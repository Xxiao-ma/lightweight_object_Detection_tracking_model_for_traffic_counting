{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set seed\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "if('tensorflow' == K.backend()):\n",
    "    import tensorflow as tf\n",
    "    from keras.backend.tensorflow_backend import set_session\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "    \n",
    "from math import ceil\n",
    "#network\n",
    "from models.keras_mobilenet_v2_ssdlite_lstm_drop19and1fm_6 import mobilenet_v2_ssd\n",
    "from losses.keras_ssd_loss import SSDLoss\n",
    "from data_generatornowinuse.object_detection_2d_data_generator_lstm import DataGenerator\n",
    "from data_generatornowinuse.object_detection_2d_geometric_ops import Resize\n",
    "from data_generatornowinuse.object_detection_2d_photometric_ops import ConvertTo3Channels\n",
    "from data_generatornowinuse.data_augmentation_chain_original_ssd import SSDDataAugmentation\n",
    "from utils.coco import get_coco_category_maps\n",
    "from utils.ssd_input_encoder import SSDInputEncoder\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, LearningRateScheduler, TerminateOnNaN, CSVLogger\n",
    "from matplotlib import pyplot as plt\n",
    "from ssd_encoder_decoder.ssd_output_decoder import decode_detections, decode_detections_fast\n",
    "from data_generator.object_detection_2d_misc_utils import apply_inverse_transforms\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SSD configuation\n",
    "\n",
    "img_height = 300 # Height of the model input images\n",
    "img_width = 300 # Width of the model input images\n",
    "img_channels = 3 # Number of color channels of the model input images\n",
    "timesteps = 1 # Time step for sequence input frames\n",
    "#mean_color = [123, 117, 104] # The per-channel mean of the images in the dataset. Do not change this value if you're using any of the pre-trained weights.\n",
    "#swap_channels = [2, 1, 0] # The color channel order in the original SSD is BGR, so we'll have the model reverse the color channel order of the input images.\n",
    "n_classes = 7 # Number of positive classes, e.g. 20 for Pascal VOC, 80 for MS COCO\n",
    "\n",
    "#??what decide the anchor box scaling factor??\n",
    "#scale，对于每个featuremap 它的anchor的计算 Sk = Smin +[(Smax - Smin)/(m-1)]*(k-1)\n",
    "#其中Smin默认是0.2,表示最低层的scale为0.2,默认Smax 为0.9,同时也拥有长宽比alpha，所以能求得每个anchor的宽Sk*sqr(alpha)和高Sk/sqr(alpha)\n",
    "#默认 m=6 ， scale:[0.2,0.34,0.48,0.62,0.76,0.9]\n",
    "#结果乘以图片实际款高即可得到anchor的实际大小\n",
    "scales_pascal = [0.2, 0.37, 0.54, 0.71, 0.88] # min 0.1 max 1.05 The anchor box scaling factors used in the original SSD300 for the Pascal VOC datasets\n",
    " # min 0.1 max 1.05 The anchor box scaling factors used in the original SSD300 for the Pascal VOC datasets\n",
    "scales_coco = [0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05] # The anchor box scaling factors used in the original SSD300 for the MS COCO datasets\n",
    "scales = scales_pascal\n",
    "#长宽比# 6 6 6 4 \n",
    "aspect_ratios = [[1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5]] # The anchor box aspect ratios used in the original SSD300; the order matters\n",
    "\n",
    "two_boxes_for_ar1 = True\n",
    "steps = [16, 32, 64, 100] # 特征图cell的大小The space between two adjacent anchor box center points for each predictor layer.\n",
    "offsets = [ 0.5, 0.5, 0.5, 0.5] # 偏移值，用来确定先验框中心The offsets of the first anchor box center points from the top and left borders of the image as a fraction of the step size for each predictor layer.\n",
    "clip_boxes = False # Whether or not to clip the anchor boxes to lie entirely within the image boundaries\n",
    "variances = [0.1, 0.1, 0.2, 0.2] # The variances by which the encoded target coordinates are divided as in the original implementation\n",
    "normalize_coords = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/xiaoxiao/anaconda3/envs/tf13/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xiaoxiao/anaconda3/envs/tf13/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xiaoxiao/anaconda3/envs/tf13/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:102: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "changed scale\n",
      "WARNING:tensorflow:From /home/xiaoxiao/anaconda3/envs/tf13/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xiaoxiao/anaconda3/envs/tf13/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xiaoxiao/anaconda3/envs/tf13/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xiaoxiao/anaconda3/envs/tf13/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xiaoxiao/anaconda3/envs/tf13/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xiaoxiao/anaconda3/envs/tf13/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xiaoxiao/anaconda3/envs/tf13/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xiaoxiao/anaconda3/envs/tf13/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xiaoxiao/ssdkeras/project_in_git/MobileNetv2_SSDlite/losses/keras_ssd_loss.py:87: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xiaoxiao/ssdkeras/project_in_git/MobileNetv2_SSDlite/losses/keras_ssd_loss.py:129: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From /home/xiaoxiao/ssdkeras/project_in_git/MobileNetv2_SSDlite/losses/keras_ssd_loss.py:68: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/xiaoxiao/ssdkeras/project_in_git/MobileNetv2_SSDlite/losses/keras_ssd_loss.py:170: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "img_input (InputLayer)          (10, 1, 300, 300, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (10, 1, 302, 301, 3) 0           img_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (10, 1, 150, 150, 32 864         time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (10, 1, 150, 150, 32 128         time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (10, 1, 150, 150, 32 0           time_distributed_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_5 (TimeDistrib (10, 1, 150, 150, 32 288         time_distributed_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_6 (TimeDistrib (10, 1, 150, 150, 32 128         time_distributed_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_7 (TimeDistrib (10, 1, 150, 150, 32 0           time_distributed_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_8 (TimeDistrib (10, 1, 150, 150, 16 512         time_distributed_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_9 (TimeDistrib (10, 1, 150, 150, 16 64          time_distributed_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_10 (TimeDistri (10, 1, 150, 150, 96 1536        time_distributed_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_11 (TimeDistri (10, 1, 150, 150, 96 384         time_distributed_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_12 (TimeDistri (10, 1, 150, 150, 96 0           time_distributed_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_13 (TimeDistri (10, 1, 152, 151, 96 0           time_distributed_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_14 (TimeDistri (10, 1, 75, 75, 96)  864         time_distributed_13[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_15 (TimeDistri (10, 1, 75, 75, 96)  384         time_distributed_14[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_16 (TimeDistri (10, 1, 75, 75, 96)  0           time_distributed_15[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_17 (TimeDistri (10, 1, 75, 75, 24)  2304        time_distributed_16[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_18 (TimeDistri (10, 1, 75, 75, 24)  96          time_distributed_17[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_19 (TimeDistri (10, 1, 75, 75, 144) 3456        time_distributed_18[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_20 (TimeDistri (10, 1, 75, 75, 144) 576         time_distributed_19[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_21 (TimeDistri (10, 1, 75, 75, 144) 0           time_distributed_20[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_22 (TimeDistri (10, 1, 75, 75, 144) 1296        time_distributed_21[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_23 (TimeDistri (10, 1, 75, 75, 144) 576         time_distributed_22[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_24 (TimeDistri (10, 1, 75, 75, 144) 0           time_distributed_23[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_25 (TimeDistri (10, 1, 75, 75, 24)  3456        time_distributed_24[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_26 (TimeDistri (10, 1, 75, 75, 24)  96          time_distributed_25[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage2_block2_add (Add)     (10, 1, 75, 75, 24)  0           time_distributed_18[0][0]        \n",
      "                                                                 time_distributed_26[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_27 (TimeDistri (10, 1, 75, 75, 144) 3456        bbn_stage2_block2_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_28 (TimeDistri (10, 1, 75, 75, 144) 576         time_distributed_27[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_29 (TimeDistri (10, 1, 75, 75, 144) 0           time_distributed_28[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_30 (TimeDistri (10, 1, 77, 77, 144) 0           time_distributed_29[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_31 (TimeDistri (10, 1, 38, 38, 144) 1296        time_distributed_30[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_32 (TimeDistri (10, 1, 38, 38, 144) 576         time_distributed_31[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_33 (TimeDistri (10, 1, 38, 38, 144) 0           time_distributed_32[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_34 (TimeDistri (10, 1, 38, 38, 32)  4608        time_distributed_33[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_35 (TimeDistri (10, 1, 38, 38, 32)  128         time_distributed_34[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_36 (TimeDistri (10, 1, 38, 38, 192) 6144        time_distributed_35[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_37 (TimeDistri (10, 1, 38, 38, 192) 768         time_distributed_36[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_38 (TimeDistri (10, 1, 38, 38, 192) 0           time_distributed_37[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_39 (TimeDistri (10, 1, 38, 38, 192) 1728        time_distributed_38[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_40 (TimeDistri (10, 1, 38, 38, 192) 768         time_distributed_39[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_41 (TimeDistri (10, 1, 38, 38, 192) 0           time_distributed_40[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_42 (TimeDistri (10, 1, 38, 38, 32)  6144        time_distributed_41[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_43 (TimeDistri (10, 1, 38, 38, 32)  128         time_distributed_42[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage3_block2_add (Add)     (10, 1, 38, 38, 32)  0           time_distributed_35[0][0]        \n",
      "                                                                 time_distributed_43[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_44 (TimeDistri (10, 1, 38, 38, 192) 6144        bbn_stage3_block2_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_45 (TimeDistri (10, 1, 38, 38, 192) 768         time_distributed_44[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_46 (TimeDistri (10, 1, 38, 38, 192) 0           time_distributed_45[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_47 (TimeDistri (10, 1, 38, 38, 192) 1728        time_distributed_46[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_48 (TimeDistri (10, 1, 38, 38, 192) 768         time_distributed_47[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_49 (TimeDistri (10, 1, 38, 38, 192) 0           time_distributed_48[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_50 (TimeDistri (10, 1, 38, 38, 32)  6144        time_distributed_49[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_51 (TimeDistri (10, 1, 38, 38, 32)  128         time_distributed_50[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage3_block3_add (Add)     (10, 1, 38, 38, 32)  0           bbn_stage3_block2_add[0][0]      \n",
      "                                                                 time_distributed_51[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_52 (TimeDistri (10, 1, 38, 38, 192) 6144        bbn_stage3_block3_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_53 (TimeDistri (10, 1, 38, 38, 192) 768         time_distributed_52[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_54 (TimeDistri (10, 1, 38, 38, 192) 0           time_distributed_53[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_55 (TimeDistri (10, 1, 40, 39, 192) 0           time_distributed_54[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_56 (TimeDistri (10, 1, 19, 19, 192) 1728        time_distributed_55[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_57 (TimeDistri (10, 1, 19, 19, 192) 768         time_distributed_56[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_58 (TimeDistri (10, 1, 19, 19, 192) 0           time_distributed_57[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_59 (TimeDistri (10, 1, 19, 19, 64)  12288       time_distributed_58[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_60 (TimeDistri (10, 1, 19, 19, 64)  256         time_distributed_59[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_61 (TimeDistri (10, 1, 19, 19, 384) 24576       time_distributed_60[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_62 (TimeDistri (10, 1, 19, 19, 384) 1536        time_distributed_61[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_63 (TimeDistri (10, 1, 19, 19, 384) 0           time_distributed_62[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_64 (TimeDistri (10, 1, 19, 19, 384) 3456        time_distributed_63[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_65 (TimeDistri (10, 1, 19, 19, 384) 1536        time_distributed_64[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_66 (TimeDistri (10, 1, 19, 19, 384) 0           time_distributed_65[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_67 (TimeDistri (10, 1, 19, 19, 64)  24576       time_distributed_66[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_68 (TimeDistri (10, 1, 19, 19, 64)  256         time_distributed_67[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block2_add (Add)     (10, 1, 19, 19, 64)  0           time_distributed_60[0][0]        \n",
      "                                                                 time_distributed_68[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_69 (TimeDistri (10, 1, 19, 19, 384) 24576       bbn_stage4_block2_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_70 (TimeDistri (10, 1, 19, 19, 384) 1536        time_distributed_69[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_71 (TimeDistri (10, 1, 19, 19, 384) 0           time_distributed_70[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_72 (TimeDistri (10, 1, 19, 19, 384) 3456        time_distributed_71[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_73 (TimeDistri (10, 1, 19, 19, 384) 1536        time_distributed_72[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_74 (TimeDistri (10, 1, 19, 19, 384) 0           time_distributed_73[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_75 (TimeDistri (10, 1, 19, 19, 64)  24576       time_distributed_74[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_76 (TimeDistri (10, 1, 19, 19, 64)  256         time_distributed_75[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block3_add (Add)     (10, 1, 19, 19, 64)  0           bbn_stage4_block2_add[0][0]      \n",
      "                                                                 time_distributed_76[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_77 (TimeDistri (10, 1, 19, 19, 384) 24576       bbn_stage4_block3_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_78 (TimeDistri (10, 1, 19, 19, 384) 1536        time_distributed_77[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_79 (TimeDistri (10, 1, 19, 19, 384) 0           time_distributed_78[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_80 (TimeDistri (10, 1, 19, 19, 384) 3456        time_distributed_79[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_81 (TimeDistri (10, 1, 19, 19, 384) 1536        time_distributed_80[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_82 (TimeDistri (10, 1, 19, 19, 384) 0           time_distributed_81[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_83 (TimeDistri (10, 1, 19, 19, 64)  24576       time_distributed_82[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_84 (TimeDistri (10, 1, 19, 19, 64)  256         time_distributed_83[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block4_add (Add)     (10, 1, 19, 19, 64)  0           bbn_stage4_block3_add[0][0]      \n",
      "                                                                 time_distributed_84[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_85 (TimeDistri (10, 1, 19, 19, 384) 24576       bbn_stage4_block4_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_86 (TimeDistri (10, 1, 19, 19, 384) 1536        time_distributed_85[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_87 (TimeDistri (10, 1, 19, 19, 384) 0           time_distributed_86[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_88 (TimeDistri (10, 1, 19, 19, 384) 3456        time_distributed_87[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_89 (TimeDistri (10, 1, 19, 19, 384) 1536        time_distributed_88[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_90 (TimeDistri (10, 1, 19, 19, 384) 0           time_distributed_89[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_91 (TimeDistri (10, 1, 19, 19, 96)  36864       time_distributed_90[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_92 (TimeDistri (10, 1, 19, 19, 96)  384         time_distributed_91[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_93 (TimeDistri (10, 1, 19, 19, 576) 55296       time_distributed_92[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_94 (TimeDistri (10, 1, 19, 19, 576) 2304        time_distributed_93[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_95 (TimeDistri (10, 1, 19, 19, 576) 0           time_distributed_94[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_96 (TimeDistri (10, 1, 19, 19, 576) 5184        time_distributed_95[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_97 (TimeDistri (10, 1, 19, 19, 576) 2304        time_distributed_96[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_98 (TimeDistri (10, 1, 19, 19, 576) 0           time_distributed_97[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_99 (TimeDistri (10, 1, 19, 19, 96)  55296       time_distributed_98[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_100 (TimeDistr (10, 1, 19, 19, 96)  384         time_distributed_99[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block6_add (Add)     (10, 1, 19, 19, 96)  0           time_distributed_92[0][0]        \n",
      "                                                                 time_distributed_100[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_101 (TimeDistr (10, 1, 19, 19, 576) 55296       bbn_stage4_block6_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_102 (TimeDistr (10, 1, 19, 19, 576) 2304        time_distributed_101[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_103 (TimeDistr (10, 1, 19, 19, 576) 0           time_distributed_102[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_104 (TimeDistr (10, 1, 19, 19, 576) 5184        time_distributed_103[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_105 (TimeDistr (10, 1, 19, 19, 576) 2304        time_distributed_104[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_106 (TimeDistr (10, 1, 19, 19, 576) 0           time_distributed_105[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_107 (TimeDistr (10, 1, 19, 19, 96)  55296       time_distributed_106[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_108 (TimeDistr (10, 1, 19, 19, 96)  384         time_distributed_107[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block7_add (Add)     (10, 1, 19, 19, 96)  0           bbn_stage4_block6_add[0][0]      \n",
      "                                                                 time_distributed_108[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_109 (TimeDistr (10, 1, 19, 19, 576) 55296       bbn_stage4_block7_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_110 (TimeDistr (10, 1, 19, 19, 576) 2304        time_distributed_109[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_111 (TimeDistr (10, 1, 19, 19, 576) 0           time_distributed_110[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_112 (TimeDistr (10, 1, 21, 21, 576) 0           time_distributed_111[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_113 (TimeDistr (10, 1, 10, 10, 576) 5184        time_distributed_112[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_114 (TimeDistr (10, 1, 10, 10, 576) 2304        time_distributed_113[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_115 (TimeDistr (10, 1, 10, 10, 576) 0           time_distributed_114[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_116 (TimeDistr (10, 1, 10, 10, 160) 92160       time_distributed_115[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_117 (TimeDistr (10, 1, 10, 10, 160) 640         time_distributed_116[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_118 (TimeDistr (10, 1, 10, 10, 960) 153600      time_distributed_117[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_119 (TimeDistr (10, 1, 10, 10, 960) 3840        time_distributed_118[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_120 (TimeDistr (10, 1, 10, 10, 960) 0           time_distributed_119[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_121 (TimeDistr (10, 1, 10, 10, 960) 8640        time_distributed_120[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_122 (TimeDistr (10, 1, 10, 10, 960) 3840        time_distributed_121[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_123 (TimeDistr (10, 1, 10, 10, 960) 0           time_distributed_122[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_124 (TimeDistr (10, 1, 10, 10, 160) 153600      time_distributed_123[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_125 (TimeDistr (10, 1, 10, 10, 160) 640         time_distributed_124[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage5_block2_add (Add)     (10, 1, 10, 10, 160) 0           time_distributed_117[0][0]       \n",
      "                                                                 time_distributed_125[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_126 (TimeDistr (10, 1, 10, 10, 960) 153600      bbn_stage5_block2_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_127 (TimeDistr (10, 1, 10, 10, 960) 3840        time_distributed_126[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_128 (TimeDistr (10, 1, 10, 10, 960) 0           time_distributed_127[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_129 (TimeDistr (10, 1, 10, 10, 960) 8640        time_distributed_128[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_130 (TimeDistr (10, 1, 10, 10, 960) 3840        time_distributed_129[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_131 (TimeDistr (10, 1, 10, 10, 960) 0           time_distributed_130[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_132 (TimeDistr (10, 1, 10, 10, 160) 153600      time_distributed_131[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_133 (TimeDistr (10, 1, 10, 10, 160) 640         time_distributed_132[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage5_block3_add (Add)     (10, 1, 10, 10, 160) 0           bbn_stage5_block2_add[0][0]      \n",
      "                                                                 time_distributed_133[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_134 (TimeDistr (10, 1, 10, 10, 960) 153600      bbn_stage5_block3_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_135 (TimeDistr (10, 1, 10, 10, 960) 3840        time_distributed_134[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_136 (TimeDistr (10, 1, 10, 10, 960) 0           time_distributed_135[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_137 (TimeDistr (10, 1, 10, 10, 960) 8640        time_distributed_136[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_138 (TimeDistr (10, 1, 10, 10, 960) 3840        time_distributed_137[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_139 (TimeDistr (10, 1, 10, 10, 960) 0           time_distributed_138[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_140 (TimeDistr (10, 1, 10, 10, 320) 307200      time_distributed_139[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_141 (TimeDistr (10, 1, 10, 10, 320) 1280        time_distributed_140[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_lst_m2d_1 (Bottlenec (10, 1, 10, 10, 80)  64240       time_distributed_141[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_142 (TimeDistr (10, 1, 10, 10, 1280 102400      bottleneck_lst_m2d_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_143 (TimeDistr (10, 1, 10, 10, 1280 5120        time_distributed_142[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "ssd_2_conv_relu (ReLU)          (10, 1, 10, 10, 1280 0           time_distributed_143[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_144 (TimeDistr (10, 1, 10, 10, 256) 327680      ssd_2_conv_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_145 (TimeDistr (10, 1, 10, 10, 256) 1024        time_distributed_144[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_146 (TimeDistr (10, 1, 10, 10, 256) 0           time_distributed_145[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_147 (TimeDistr (10, 1, 12, 11, 256) 0           time_distributed_146[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_148 (TimeDistr (10, 1, 5, 5, 256)   2304        time_distributed_147[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_149 (TimeDistr (10, 1, 5, 5, 256)   1024        time_distributed_148[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_150 (TimeDistr (10, 1, 5, 5, 256)   0           time_distributed_149[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_151 (TimeDistr (10, 1, 5, 5, 512)   131072      time_distributed_150[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_152 (TimeDistr (10, 1, 5, 5, 512)   2048        time_distributed_151[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_153 (TimeDistr (10, 1, 5, 5, 512)   0           time_distributed_152[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_154 (TimeDistr (10, 1, 5, 5, 128)   65536       time_distributed_153[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_155 (TimeDistr (10, 1, 5, 5, 128)   512         time_distributed_154[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_156 (TimeDistr (10, 1, 5, 5, 128)   0           time_distributed_155[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_157 (TimeDistr (10, 1, 7, 7, 128)   0           time_distributed_156[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_158 (TimeDistr (10, 1, 3, 3, 128)   1152        time_distributed_157[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_159 (TimeDistr (10, 1, 3, 3, 128)   512         time_distributed_158[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_160 (TimeDistr (10, 1, 3, 3, 128)   0           time_distributed_159[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_161 (TimeDistr (10, 1, 3, 3, 256)   32768       time_distributed_160[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_162 (TimeDistr (10, 1, 3, 3, 256)   1024        time_distributed_161[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_163 (TimeDistr (10, 1, 3, 3, 256)   0           time_distributed_162[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_164 (TimeDistr (10, 1, 3, 3, 128)   32768       time_distributed_163[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_165 (TimeDistr (10, 1, 3, 3, 128)   512         time_distributed_164[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_166 (TimeDistr (10, 1, 3, 3, 128)   0           time_distributed_165[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_167 (TimeDistr (10, 1, 5, 5, 128)   0           time_distributed_166[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_168 (TimeDistr (10, 1, 2, 2, 128)   1152        time_distributed_167[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_169 (TimeDistr (10, 1, 2, 2, 128)   512         time_distributed_168[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_170 (TimeDistr (10, 1, 2, 2, 128)   0           time_distributed_169[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_171 (TimeDistr (10, 1, 2, 2, 256)   32768       time_distributed_170[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_172 (TimeDistr (10, 1, 2, 2, 256)   1024        time_distributed_171[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_173 (TimeDistr (10, 1, 2, 2, 256)   0           time_distributed_172[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_174 (TimeDistr (10, 1, 10, 10, 1280 11520       ssd_2_conv_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_179 (TimeDistr (10, 1, 5, 5, 512)   4608        time_distributed_153[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_184 (TimeDistr (10, 1, 3, 3, 256)   2304        time_distributed_163[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_189 (TimeDistr (10, 1, 2, 2, 256)   2304        time_distributed_173[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_194 (TimeDistr (10, 1, 10, 10, 1280 11520       ssd_2_conv_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_199 (TimeDistr (10, 1, 5, 5, 512)   4608        time_distributed_153[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_204 (TimeDistr (10, 1, 3, 3, 256)   2304        time_distributed_163[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_209 (TimeDistr (10, 1, 2, 2, 256)   2304        time_distributed_173[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_175 (TimeDistr (10, 1, 10, 10, 1280 5120        time_distributed_174[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_180 (TimeDistr (10, 1, 5, 5, 512)   2048        time_distributed_179[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_185 (TimeDistr (10, 1, 3, 3, 256)   1024        time_distributed_184[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_190 (TimeDistr (10, 1, 2, 2, 256)   1024        time_distributed_189[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_195 (TimeDistr (10, 1, 10, 10, 1280 5120        time_distributed_194[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_200 (TimeDistr (10, 1, 5, 5, 512)   2048        time_distributed_199[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_205 (TimeDistr (10, 1, 3, 3, 256)   1024        time_distributed_204[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_210 (TimeDistr (10, 1, 2, 2, 256)   1024        time_distributed_209[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_176 (TimeDistr (10, 1, 10, 10, 1280 0           time_distributed_175[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_181 (TimeDistr (10, 1, 5, 5, 512)   0           time_distributed_180[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_186 (TimeDistr (10, 1, 3, 3, 256)   0           time_distributed_185[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_191 (TimeDistr (10, 1, 2, 2, 256)   0           time_distributed_190[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_196 (TimeDistr (10, 1, 10, 10, 1280 0           time_distributed_195[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_201 (TimeDistr (10, 1, 5, 5, 512)   0           time_distributed_200[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_206 (TimeDistr (10, 1, 3, 3, 256)   0           time_distributed_205[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_211 (TimeDistr (10, 1, 2, 2, 256)   0           time_distributed_210[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_177 (TimeDistr (10, 1, 10, 10, 48)  61440       time_distributed_176[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_182 (TimeDistr (10, 1, 5, 5, 48)    24576       time_distributed_181[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_187 (TimeDistr (10, 1, 3, 3, 48)    12288       time_distributed_186[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_192 (TimeDistr (10, 1, 2, 2, 32)    8192        time_distributed_191[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_197 (TimeDistr (10, 1, 10, 10, 24)  30720       time_distributed_196[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_202 (TimeDistr (10, 1, 5, 5, 24)    12288       time_distributed_201[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_207 (TimeDistr (10, 1, 3, 3, 24)    6144        time_distributed_206[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_212 (TimeDistr (10, 1, 2, 2, 16)    4096        time_distributed_211[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_178 (TimeDistr (10, 1, 10, 10, 48)  192         time_distributed_177[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_183 (TimeDistr (10, 1, 5, 5, 48)    192         time_distributed_182[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_188 (TimeDistr (10, 1, 3, 3, 48)    192         time_distributed_187[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_193 (TimeDistr (10, 1, 2, 2, 32)    128         time_distributed_192[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_198 (TimeDistr (10, 1, 10, 10, 24)  96          time_distributed_197[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_203 (TimeDistr (10, 1, 5, 5, 24)    96          time_distributed_202[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_208 (TimeDistr (10, 1, 3, 3, 24)    96          time_distributed_207[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_213 (TimeDistr (10, 1, 2, 2, 16)    64          time_distributed_212[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "ssd_cls2_reshape (Reshape)      (10, 600, 8)         0           time_distributed_178[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "ssd_cls3_reshape (Reshape)      (10, 150, 8)         0           time_distributed_183[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "ssd_cls4_reshape (Reshape)      (10, 54, 8)          0           time_distributed_188[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "ssd_cls5_reshape (Reshape)      (10, 16, 8)          0           time_distributed_193[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_214 (TimeDistr (10, 1, 10, 10, 6, 8 0           time_distributed_198[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_215 (TimeDistr (10, 1, 5, 5, 6, 8)  0           time_distributed_203[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_216 (TimeDistr (10, 1, 3, 3, 6, 8)  0           time_distributed_208[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_217 (TimeDistr (10, 1, 2, 2, 4, 8)  0           time_distributed_213[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "ssd_cls (Concatenate)           (10, 820, 8)         0           ssd_cls2_reshape[0][0]           \n",
      "                                                                 ssd_cls3_reshape[0][0]           \n",
      "                                                                 ssd_cls4_reshape[0][0]           \n",
      "                                                                 ssd_cls5_reshape[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_box2_reshape (Reshape)      (10, 600, 4)         0           time_distributed_198[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "ssd_box3_reshape (Reshape)      (10, 150, 4)         0           time_distributed_203[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "ssd_box4_reshape (Reshape)      (10, 54, 4)          0           time_distributed_208[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "ssd_box5_reshape (Reshape)      (10, 16, 4)          0           time_distributed_213[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "ssd_priorbox2_reshape (Reshape) (10, 600, 8)         0           time_distributed_214[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "ssd_priorbox3_reshape (Reshape) (10, 150, 8)         0           time_distributed_215[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "ssd_priorbox4_reshape (Reshape) (10, 54, 8)          0           time_distributed_216[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "ssd_priorbox5_reshape (Reshape) (10, 16, 8)          0           time_distributed_217[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "ssd_mbox_conf_softmax (Activati (10, 820, 8)         0           ssd_cls[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "ssd_box (Concatenate)           (10, 820, 4)         0           ssd_box2_reshape[0][0]           \n",
      "                                                                 ssd_box3_reshape[0][0]           \n",
      "                                                                 ssd_box4_reshape[0][0]           \n",
      "                                                                 ssd_box5_reshape[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_priorbox (Concatenate)      (10, 820, 8)         0           ssd_priorbox2_reshape[0][0]      \n",
      "                                                                 ssd_priorbox3_reshape[0][0]      \n",
      "                                                                 ssd_priorbox4_reshape[0][0]      \n",
      "                                                                 ssd_priorbox5_reshape[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "ssd_predictions (Concatenate)   (10, 820, 20)        0           ssd_mbox_conf_softmax[0][0]      \n",
      "                                                                 ssd_box[0][0]                    \n",
      "                                                                 ssd_priorbox[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 2,871,120\n",
      "Trainable params: 2,823,168\n",
      "Non-trainable params: 47,952\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Build the model\n",
    "K.clear_session()\n",
    "\n",
    "model = mobilenet_v2_ssd(image_size=(img_height, img_width, img_channels),\n",
    "                timesteps=timesteps,\n",
    "                n_classes=n_classes,\n",
    "                mode='training',\n",
    "                l2_regularization=0.0005,\n",
    "                scales=scales,\n",
    "                aspect_ratios_per_layer=aspect_ratios,\n",
    "                two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                steps=steps,\n",
    "                offsets=offsets,\n",
    "                clip_boxes=clip_boxes,\n",
    "                variances=variances,\n",
    "                normalize_coords=normalize_coords)\n",
    "# train from scratch.0, 2.0, 0.5, 3.0, 1.0/3.0],h, no weights to load\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
    "# set_trainable(r\"(ssd\\_[cls|box].*)\", model)\n",
    "model.compile(optimizer=adam, loss=ssd_loss.compute_loss)\n",
    "\n",
    "print(model.summary())\n",
    "#Total params: 3,160,240\n",
    "#Trainable params: 3,108,888"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image set 'train_seqs_list_forBottleneckLSTM.txt': 100%|██████████| 10790/10790 [00:12<00:00, 842.12it/s]\n",
      "Processing image set 'val_for_lstm.txt': 100%|██████████| 820/820 [00:00<00:00, 1205.90it/s]\n",
      "Creating HDF5 dataset: 100%|██████████| 10790/10790 [00:16<00:00, 635.42it/s]\n",
      "Creating HDF5 dataset: 100%|██████████| 520/520 [00:00<00:00, 945.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# 1: Instantiate two `DataGenerator` objects: One for training, one for validation.\n",
    "\n",
    "# Optional: If you have enough memory, consider loading the images into memory for the reasons explained above.\n",
    "\n",
    "train_dataset = DataGenerator()#(load_images_into_memory=False, hdf5_dataset_path=None)\n",
    "val_dataset = DataGenerator()#(load_images_into_memory=False, hdf5_dataset_path=None)\n",
    "\n",
    "# 2: Parse the image and label lists for the training and validation datasets. This can take a while.\n",
    "\n",
    "# TODO: Set the paths to the datasets here.\n",
    "\n",
    "# The directories that contain the images.\n",
    "VOC_2007_images_dir = './data_index/data/pic/'\n",
    "\n",
    "# The directories that contain the annotations.\n",
    "VOC_2007_annotations_dir = './data_index/data/label/'\n",
    "#train and val_for_lstm are modified to make the size of datasets fit the batch size of lstm model\n",
    "#VOC_2007_trainval_image_set_filename = './data_index/data/train_for_lstm.txt'\n",
    "VOC_2007_trainval_image_set_filename = './data_index/data/train_seqs_list_forBottleneckLSTM.txt'\n",
    "\n",
    "#VOC_2012_trainval_image_set_filename = '../../datasets/VOCdevkit/VOC2012/ImageSets/Main/trainval.txt'\n",
    "VOC_2007_test_image_set_filename     = './data_index/data/val_for_lstm.txt'\n",
    "\n",
    "# The XML parser needs to now what object class names to look for and in which order to map them to integers.\n",
    "#classes = ['background','human','bicycle','truck','car','bus','motorbike','escooter']\n",
    "classes = ['background','human','bicycle','truck','car','bus','escooter','motorbike']\n",
    "\n",
    "\n",
    "train_dataset.parse_xml(images_dirs=[VOC_2007_images_dir],\n",
    "                        image_set_filenames=[VOC_2007_trainval_image_set_filename],\n",
    "                        annotations_dirs=[VOC_2007_annotations_dir],\n",
    "                        classes=classes,\n",
    "                        include_classes='all',\n",
    "                        exclude_truncated=False,\n",
    "                        exclude_difficult=False,\n",
    "                        ret=False)\n",
    "\n",
    "val_dataset.parse_xml(images_dirs=[VOC_2007_images_dir],\n",
    "                      image_set_filenames=[VOC_2007_test_image_set_filename],\n",
    "                      annotations_dirs=[VOC_2007_annotations_dir],\n",
    "                      classes=classes,\n",
    "                      include_classes='all',\n",
    "                      exclude_truncated=False,\n",
    "                      exclude_difficult=False,#used to be True\n",
    "                      ret=False)\n",
    "\n",
    "# Optional: Convert the dataset into an HDF5 dataset. This will require more disk space, but will\n",
    "# speed up the training. Doing this is not relevant in case you activated the `load_images_into_memory`\n",
    "# option in the constructor, because in that cas the images are in memory already anyway. If you don't\n",
    "# want to create HDF5 datasets, comment out the subsequent two function calls.\n",
    "\n",
    "train_dataset.create_hdf5_dataset(file_path='dataset_pascal_voc_07_trainval.h5',\n",
    "                                  resize=False,\n",
    "                                  variable_image_size=True,\n",
    "                                  verbose=True)\n",
    "\n",
    "val_dataset.create_hdf5_dataset(file_path='dataset_pascal_voc_07_test.h5',\n",
    "                                resize=False,\n",
    "                                variable_image_size=True,\n",
    "                                verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate schedule\n",
    "def lr_schedule(epoch):\n",
    "    if epoch < 200:\n",
    "        return 0.001\n",
    "    elif epoch < 500:\n",
    "        return 0.0001\n",
    "    else:\n",
    "        return 0.00001\n",
    "\n",
    "\n",
    "# set trainable layers\n",
    "def set_trainable(layer_regex, keras_model=None, indent=0, verbose=1):\n",
    "    # In multi-GPU training, we wrap the model. Get layers\n",
    "    # of the inner model because they have the weights.\n",
    "    layers = keras_model.inner_model.layers if hasattr(keras_model, \"inner_model\") \\\n",
    "        else keras_model.layers\n",
    "\n",
    "    for layer in layers:\n",
    "        # Is the layer a model?\n",
    "        if layer.__class__.__name__ == 'Model':\n",
    "            print(\"In model: \", layer.name)\n",
    "            set_trainable(\n",
    "                layer_regex, keras_model=layer)\n",
    "            continue\n",
    "\n",
    "        if not layer.weights:\n",
    "            continue\n",
    "        # Is it trainable?\n",
    "        trainable = bool(re.fullmatch(layer_regex, layer.name))\n",
    "        # Update layer. If layer is a container, update inner layer.\n",
    "        if layer.__class__.__name__ == 'TimeDistributed':\n",
    "            layer.layer.trainable = trainable\n",
    "        else:\n",
    "            layer.trainable = trainable\n",
    "        # Print trainable layer names\n",
    "        if trainable and verbose > 0:\n",
    "            print(\"{}{:20}   ({})\".format(\" \" * indent, layer.name, layer.__class__.__name__))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3: Set the batch size.\n",
    "\n",
    "batch_size = 10 # Change the batch size if you like, or if you run into GPU memory issues.\n",
    "\n",
    "# 4: Set the train_datasetimage transformations for pre-processing and data augmentation options.\n",
    "\n",
    "# For the training generator:\n",
    "# no extra \n",
    "ssd_data_augmentation = SSDDataAugmentation(img_height=img_height,\n",
    "                                            img_width=img_width)\n",
    "# learning rate schedule\n",
    "def lr_schedule(epoch):\n",
    "    if epoch < 200:\n",
    "        return 0.001\n",
    "    elif epoch < 500:\n",
    "        return 0.0001\n",
    "    else:\n",
    "        return 0.00001\n",
    "\n",
    "\n",
    "# set trainable layers\n",
    "def set_trainable(layer_regex, keras_model=None, indent=0, verbose=1):\n",
    "    # In multi-GPU training, we wrap the model. Get layers\n",
    "    # of the inner model because they have the weights.\n",
    "    layers = keras_model.inner_model.layers if hasattr(keras_model, \"inner_model\") \\\n",
    "        else keras_model.layers\n",
    "\n",
    "    for layer in layers:\n",
    "        # Is the layer a model?\n",
    "        if layer.__class__.__name__ == 'Model':\n",
    "            print(\"In model: \", layer.name)\n",
    "            set_trainable(\n",
    "                layer_regex, keras_model=layer)\n",
    "            continue\n",
    "\n",
    "        if not layer.weights:\n",
    "            continue\n",
    "        # Is it trainable?\n",
    "        trainable = bool(re.fullmatch(layer_regex, layer.name))\n",
    "        # Update layer. If layer is a container, update inner layer.\n",
    "        if layer.__class__.__name__ == 'TimeDistributed':\n",
    "            layer.layer.trainable = trainable\n",
    "        else:\n",
    "            layer.trainable = trainable\n",
    "        # Print trainable layer names\n",
    "        if trainable and verbose > 0:\n",
    "            print(\"{}{:20}   ({})\".format(\" \" * indent, layer.name, layer.__class__.__name__))\n",
    "\n",
    "# For the validation generator:\n",
    "convert_to_3_channels = ConvertTo3Channels()\n",
    "resize = Resize(height=img_height, width=img_width)\n",
    "\n",
    "# 5: Instantiate an encoder that can encode ground truth labels into the format needed by the SSD loss function.\n",
    "#用getlayer来获取输出层的尺寸\n",
    "# The encoder constructor needs the spatial dimensions of the model's predictor layers to create the anchor boxes.\n",
    "predictor_sizes = [model.layers[-29].output_shape[2:4],\n",
    "                   model.layers[-28].output_shape[2:4],\n",
    "                   model.layers[-27].output_shape[2:4],\n",
    "                   model.layers[-26].output_shape[2:4]]\n",
    "'''\n",
    "predictor_sizes = [model.get_layer('ssd_cls1conv2_bn').output_shape[2:4],\n",
    "                   model.get_layer('ssd_cls2conv2_bn').output_shape[1:3],\n",
    "                   model.get_layer('ssd_cls3conv2_bn').output_shape[1:3],\n",
    "                   model.get_layer('ssd_cls4conv2_bn').output_shape[1:3],\n",
    "                   model.get_layer('ssd_cls5conv2_bn').output_shape[1:3],\n",
    "                   model.get_layer('ssd_cls6conv2_bn').output_shape[1:3]]\n",
    "'''\n",
    "#encoder把ground truth labels\n",
    "ssd_input_encoder = SSDInputEncoder(img_height=img_height,\n",
    "                                    img_width=img_width,\n",
    "                                    n_classes=n_classes,\n",
    "                                    predictor_sizes=predictor_sizes,\n",
    "                                    scales=scales,\n",
    "                                    aspect_ratios_per_layer=aspect_ratios,\n",
    "                                    two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                                    steps=steps,\n",
    "                                    offsets=offsets,\n",
    "                                    clip_boxes=clip_boxes,\n",
    "                                    variances=variances,\n",
    "                                    matching_type='multi',\n",
    "                                    pos_iou_threshold=0.5,\n",
    "                                    neg_iou_limit=0.5,\n",
    "                                    normalize_coords=normalize_coords)\n",
    "\n",
    "# 6: Create the generator handles that will be passed to Keras' `fit_generator()` function.\n",
    "\n",
    "train_generator = train_dataset.generate(batch_size=batch_size,\n",
    "                                         shuffle=False,\n",
    "                                         transformations=[convert_to_3_channels,\n",
    "                                                      resize],# used to be augumentation\n",
    "                                         label_encoder=ssd_input_encoder,\n",
    "                                         returns={'processed_images',\n",
    "                                                  'encoded_labels'},\n",
    "                                         keep_images_without_gt=False)\n",
    "\n",
    "val_generator = val_dataset.generate(batch_size=batch_size,\n",
    "                                     shuffle=False,\n",
    "                                     transformations=[convert_to_3_channels,\n",
    "                                                      resize],\n",
    "                                     label_encoder=ssd_input_encoder,\n",
    "                                     returns={'processed_images',\n",
    "                                              'encoded_labels'},\n",
    "                                     keep_images_without_gt=False)\n",
    "\n",
    "# Get the number of samples in the training and validations datasets.\n",
    "#train_dataset_size = train_dataset.get_dataset_size()\n",
    "#val_dataset_size   = val_dataset.get_dataset_size()\n",
    "\n",
    "#print(\"Number of images in the training dataset:\\t{:>6}\".format(train_dataset_size))\n",
    "#print(\"Number of images in the validation dataset:\\t{:>6}\".format(val_dataset_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model callbacks.\n",
    "\n",
    "# TODO: Set the filepath under which you want to save the model.\n",
    "model_checkpoint = ModelCheckpoint(filepath='./log_lstm_test/MobileNetv2_ssdLite_bottlrNeck_epoch-{epoch:02d}_loss-{loss:.4f}_val_loss-{val_loss:.4f}.h5',\n",
    "                                   monitor='loss',#used to be val_loss\n",
    "                                   verbose=1,\n",
    "                                   save_best_only=True,\n",
    "                                   save_weights_only=False,\n",
    "                                   mode='auto',\n",
    "                                   period=1)\n",
    "#model_checkpoint.best = \n",
    "\n",
    "csv_logger = CSVLogger(filename='MobileNetv2_ssdLite_training_TestBOttlenecklstm_log.csv',\n",
    "                       separator=',',\n",
    "                       append=True)\n",
    "\n",
    "learning_rate_scheduler = LearningRateScheduler(schedule=lr_schedule,\n",
    "                                                verbose=1)\n",
    "\n",
    "terminate_on_nan = TerminateOnNaN()\n",
    "\n",
    "callbacks = [model_checkpoint,\n",
    "             csv_logger,\n",
    "             learning_rate_scheduler,\n",
    "             terminate_on_nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/xiaoxiao/anaconda3/envs/tf13/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xiaoxiao/anaconda3/envs/tf13/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Epoch 1/220\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 314s 291ms/step - loss: 8.2066 - val_loss: 6.7146\n",
      "\n",
      "Epoch 00001: loss improved from inf to 8.20661, saving model to ./log_lstm_test/MobileNetv2_ssdLite_bottlrNeck_epoch-01_loss-8.2066_val_loss-6.7146.h5\n",
      "Epoch 2/220\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 247ms/step - loss: 7.4695 - val_loss: 6.5140\n",
      "\n",
      "Epoch 00002: loss improved from 8.20661 to 7.46951, saving model to ./log_lstm_test/MobileNetv2_ssdLite_bottlrNeck_epoch-02_loss-7.4695_val_loss-6.5140.h5\n",
      "Epoch 3/220\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 268s 248ms/step - loss: 7.3809 - val_loss: 6.6454\n",
      "\n",
      "Epoch 00003: loss improved from 7.46951 to 7.38095, saving model to ./log_lstm_test/MobileNetv2_ssdLite_bottlrNeck_epoch-03_loss-7.3809_val_loss-6.6454.h5\n",
      "Epoch 4/220\n",
      "\n",
      "Epoch 00004: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 268s 248ms/step - loss: 7.3263 - val_loss: 6.5865\n",
      "\n",
      "Epoch 00004: loss improved from 7.38095 to 7.32632, saving model to ./log_lstm_test/MobileNetv2_ssdLite_bottlrNeck_epoch-04_loss-7.3263_val_loss-6.5865.h5\n",
      "Epoch 5/220\n",
      "\n",
      "Epoch 00005: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.3365 - val_loss: 6.5262\n",
      "\n",
      "Epoch 00005: loss did not improve from 7.32632\n",
      "Epoch 6/220\n",
      "\n",
      "Epoch 00006: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.3475 - val_loss: 6.5187\n",
      "\n",
      "Epoch 00006: loss did not improve from 7.32632\n",
      "Epoch 7/220\n",
      "\n",
      "Epoch 00007: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.3325 - val_loss: 6.5554\n",
      "\n",
      "Epoch 00007: loss did not improve from 7.32632\n",
      "Epoch 8/220\n",
      "\n",
      "Epoch 00008: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.3032 - val_loss: 6.4191\n",
      "\n",
      "Epoch 00008: loss improved from 7.32632 to 7.30318, saving model to ./log_lstm_test/MobileNetv2_ssdLite_bottlrNeck_epoch-08_loss-7.3032_val_loss-6.4191.h5\n",
      "Epoch 9/220\n",
      "\n",
      "Epoch 00009: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 268s 248ms/step - loss: 7.3115 - val_loss: 6.4683\n",
      "\n",
      "Epoch 00009: loss did not improve from 7.30318\n",
      "Epoch 10/220\n",
      "\n",
      "Epoch 00010: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.3179 - val_loss: 6.5290\n",
      "\n",
      "Epoch 00010: loss did not improve from 7.30318\n",
      "Epoch 11/220\n",
      "\n",
      "Epoch 00011: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 268s 248ms/step - loss: 7.2779 - val_loss: 6.4707\n",
      "\n",
      "Epoch 00011: loss improved from 7.30318 to 7.27793, saving model to ./log_lstm_test/MobileNetv2_ssdLite_bottlrNeck_epoch-11_loss-7.2779_val_loss-6.4707.h5\n",
      "Epoch 12/220\n",
      "\n",
      "Epoch 00012: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 268s 248ms/step - loss: 7.2721 - val_loss: 6.4681\n",
      "\n",
      "Epoch 00012: loss improved from 7.27793 to 7.27210, saving model to ./log_lstm_test/MobileNetv2_ssdLite_bottlrNeck_epoch-12_loss-7.2721_val_loss-6.4681.h5\n",
      "Epoch 13/220\n",
      "\n",
      "Epoch 00013: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.2698 - val_loss: 6.3712\n",
      "\n",
      "Epoch 00013: loss improved from 7.27210 to 7.26979, saving model to ./log_lstm_test/MobileNetv2_ssdLite_bottlrNeck_epoch-13_loss-7.2698_val_loss-6.3712.h5\n",
      "Epoch 14/220\n",
      "\n",
      "Epoch 00014: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 268s 248ms/step - loss: 7.3008 - val_loss: 6.4247\n",
      "\n",
      "Epoch 00014: loss did not improve from 7.26979\n",
      "Epoch 15/220\n",
      "\n",
      "Epoch 00015: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 268s 248ms/step - loss: 7.3085 - val_loss: 6.4859\n",
      "\n",
      "Epoch 00015: loss did not improve from 7.26979\n",
      "Epoch 16/220\n",
      "\n",
      "Epoch 00016: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 268s 248ms/step - loss: 7.3022 - val_loss: 6.5031\n",
      "\n",
      "Epoch 00016: loss did not improve from 7.26979\n",
      "Epoch 17/220\n",
      "\n",
      "Epoch 00017: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 268s 248ms/step - loss: 7.2849 - val_loss: 6.5946\n",
      "\n",
      "Epoch 00017: loss did not improve from 7.26979\n",
      "Epoch 18/220\n",
      "\n",
      "Epoch 00018: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 268s 248ms/step - loss: 7.2405 - val_loss: 6.5815\n",
      "\n",
      "Epoch 00018: loss improved from 7.26979 to 7.24051, saving model to ./log_lstm_test/MobileNetv2_ssdLite_bottlrNeck_epoch-18_loss-7.2405_val_loss-6.5815.h5\n",
      "Epoch 19/220\n",
      "\n",
      "Epoch 00019: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 268s 248ms/step - loss: 7.2155 - val_loss: 6.4847\n",
      "\n",
      "Epoch 00019: loss improved from 7.24051 to 7.21549, saving model to ./log_lstm_test/MobileNetv2_ssdLite_bottlrNeck_epoch-19_loss-7.2155_val_loss-6.4847.h5\n",
      "Epoch 20/220\n",
      "\n",
      "Epoch 00020: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.1926 - val_loss: 6.5053\n",
      "\n",
      "Epoch 00020: loss improved from 7.21549 to 7.19264, saving model to ./log_lstm_test/MobileNetv2_ssdLite_bottlrNeck_epoch-20_loss-7.1926_val_loss-6.5053.h5\n",
      "Epoch 21/220\n",
      "\n",
      "Epoch 00021: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 268s 248ms/step - loss: 7.1757 - val_loss: 6.5305\n",
      "\n",
      "Epoch 00021: loss improved from 7.19264 to 7.17566, saving model to ./log_lstm_test/MobileNetv2_ssdLite_bottlrNeck_epoch-21_loss-7.1757_val_loss-6.5305.h5\n",
      "Epoch 22/220\n",
      "\n",
      "Epoch 00022: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 268s 248ms/step - loss: 7.1544 - val_loss: 6.5106\n",
      "\n",
      "Epoch 00022: loss improved from 7.17566 to 7.15442, saving model to ./log_lstm_test/MobileNetv2_ssdLite_bottlrNeck_epoch-22_loss-7.1544_val_loss-6.5106.h5\n",
      "Epoch 23/220\n",
      "\n",
      "Epoch 00023: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 268s 248ms/step - loss: 7.1630 - val_loss: 6.5191\n",
      "\n",
      "Epoch 00023: loss did not improve from 7.15442\n",
      "Epoch 24/220\n",
      "\n",
      "Epoch 00024: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 268s 248ms/step - loss: 7.1712 - val_loss: 6.5159\n",
      "\n",
      "Epoch 00024: loss did not improve from 7.15442\n",
      "Epoch 25/220\n",
      "\n",
      "Epoch 00025: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 268s 248ms/step - loss: 7.1585 - val_loss: 6.5462\n",
      "\n",
      "Epoch 00025: loss did not improve from 7.15442\n",
      "Epoch 26/220\n",
      "\n",
      "Epoch 00026: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 268s 249ms/step - loss: 7.1715 - val_loss: 6.5722\n",
      "\n",
      "Epoch 00026: loss did not improve from 7.15442\n",
      "Epoch 27/220\n",
      "\n",
      "Epoch 00027: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 268s 248ms/step - loss: 7.1702 - val_loss: 6.5934\n",
      "\n",
      "Epoch 00027: loss did not improve from 7.15442\n",
      "Epoch 28/220\n",
      "\n",
      "Epoch 00028: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 268s 249ms/step - loss: 7.1958 - val_loss: 6.5836\n",
      "\n",
      "Epoch 00028: loss did not improve from 7.15442\n",
      "Epoch 29/220\n",
      "\n",
      "Epoch 00029: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.1949 - val_loss: 6.5693\n",
      "\n",
      "Epoch 00029: loss did not improve from 7.15442\n",
      "Epoch 30/220\n",
      "\n",
      "Epoch 00030: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.1869 - val_loss: 6.5521\n",
      "\n",
      "Epoch 00030: loss did not improve from 7.15442\n",
      "Epoch 31/220\n",
      "\n",
      "Epoch 00031: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 247ms/step - loss: 7.2163 - val_loss: 6.5695\n",
      "\n",
      "Epoch 00031: loss did not improve from 7.15442\n",
      "Epoch 32/220\n",
      "\n",
      "Epoch 00032: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.1818 - val_loss: 6.5907\n",
      "\n",
      "Epoch 00032: loss did not improve from 7.15442\n",
      "Epoch 33/220\n",
      "\n",
      "Epoch 00033: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.2038 - val_loss: 6.5662\n",
      "\n",
      "Epoch 00033: loss did not improve from 7.15442\n",
      "Epoch 34/220\n",
      "\n",
      "Epoch 00034: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.1836 - val_loss: 6.5923\n",
      "\n",
      "Epoch 00034: loss did not improve from 7.15442\n",
      "Epoch 35/220\n",
      "\n",
      "Epoch 00035: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.1868 - val_loss: 6.5685\n",
      "\n",
      "Epoch 00035: loss did not improve from 7.15442\n",
      "Epoch 36/220\n",
      "\n",
      "Epoch 00036: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.1866 - val_loss: 6.6074\n",
      "\n",
      "Epoch 00036: loss did not improve from 7.15442\n",
      "Epoch 37/220\n",
      "\n",
      "Epoch 00037: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.1930 - val_loss: 6.5779\n",
      "\n",
      "Epoch 00037: loss did not improve from 7.15442\n",
      "Epoch 38/220\n",
      "\n",
      "Epoch 00038: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.2002 - val_loss: 6.5653\n",
      "\n",
      "Epoch 00038: loss did not improve from 7.15442\n",
      "Epoch 39/220\n",
      "\n",
      "Epoch 00039: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 266s 247ms/step - loss: 7.1944 - val_loss: 6.5641\n",
      "\n",
      "Epoch 00039: loss did not improve from 7.15442\n",
      "Epoch 40/220\n",
      "\n",
      "Epoch 00040: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.2194 - val_loss: 6.5846\n",
      "\n",
      "Epoch 00040: loss did not improve from 7.15442\n",
      "Epoch 41/220\n",
      "\n",
      "Epoch 00041: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.1820 - val_loss: 6.5850\n",
      "\n",
      "Epoch 00041: loss did not improve from 7.15442\n",
      "Epoch 42/220\n",
      "\n",
      "Epoch 00042: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.1964 - val_loss: 6.6097\n",
      "\n",
      "Epoch 00042: loss did not improve from 7.15442\n",
      "Epoch 43/220\n",
      "\n",
      "Epoch 00043: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.1520 - val_loss: 6.5481\n",
      "\n",
      "Epoch 00043: loss improved from 7.15442 to 7.15205, saving model to ./log_lstm_test/MobileNetv2_ssdLite_bottlrNeck_epoch-43_loss-7.1520_val_loss-6.5481.h5\n",
      "Epoch 44/220\n",
      "\n",
      "Epoch 00044: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 247ms/step - loss: 7.2142 - val_loss: 6.5794\n",
      "\n",
      "Epoch 00044: loss did not improve from 7.15205\n",
      "Epoch 45/220\n",
      "\n",
      "Epoch 00045: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 268s 248ms/step - loss: 7.2175 - val_loss: 6.5886\n",
      "\n",
      "Epoch 00045: loss did not improve from 7.15205\n",
      "Epoch 46/220\n",
      "\n",
      "Epoch 00046: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 247ms/step - loss: 7.2050 - val_loss: 6.5870\n",
      "\n",
      "Epoch 00046: loss did not improve from 7.15205\n",
      "Epoch 47/220\n",
      "\n",
      "Epoch 00047: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.2026 - val_loss: 6.5870\n",
      "\n",
      "Epoch 00047: loss did not improve from 7.15205\n",
      "Epoch 48/220\n",
      "\n",
      "Epoch 00048: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.2076 - val_loss: 6.5966\n",
      "\n",
      "Epoch 00048: loss did not improve from 7.15205\n",
      "Epoch 49/220\n",
      "\n",
      "Epoch 00049: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.1993 - val_loss: 6.6097\n",
      "\n",
      "Epoch 00049: loss did not improve from 7.15205\n",
      "Epoch 50/220\n",
      "\n",
      "Epoch 00050: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.2053 - val_loss: 6.6214\n",
      "\n",
      "Epoch 00050: loss did not improve from 7.15205\n",
      "Epoch 51/220\n",
      "\n",
      "Epoch 00051: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.2110 - val_loss: 6.6386\n",
      "\n",
      "Epoch 00051: loss did not improve from 7.15205\n",
      "Epoch 52/220\n",
      "\n",
      "Epoch 00052: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.2034 - val_loss: 6.6481\n",
      "\n",
      "Epoch 00052: loss did not improve from 7.15205\n",
      "Epoch 53/220\n",
      "\n",
      "Epoch 00053: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.2082 - val_loss: 6.6378\n",
      "\n",
      "Epoch 00053: loss did not improve from 7.15205\n",
      "Epoch 54/220\n",
      "\n",
      "Epoch 00054: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 247ms/step - loss: 7.2024 - val_loss: 6.6501\n",
      "\n",
      "Epoch 00054: loss did not improve from 7.15205\n",
      "Epoch 55/220\n",
      "\n",
      "Epoch 00055: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.1996 - val_loss: 6.6655\n",
      "\n",
      "Epoch 00055: loss did not improve from 7.15205\n",
      "Epoch 56/220\n",
      "\n",
      "Epoch 00056: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 268s 248ms/step - loss: 7.2023 - val_loss: 6.6777\n",
      "\n",
      "Epoch 00056: loss did not improve from 7.15205\n",
      "Epoch 57/220\n",
      "\n",
      "Epoch 00057: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.2076 - val_loss: 6.7270\n",
      "\n",
      "Epoch 00057: loss did not improve from 7.15205\n",
      "Epoch 58/220\n",
      "\n",
      "Epoch 00058: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.1900 - val_loss: 6.7229\n",
      "\n",
      "Epoch 00058: loss did not improve from 7.15205\n",
      "Epoch 59/220\n",
      "\n",
      "Epoch 00059: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.1965 - val_loss: 6.7192\n",
      "\n",
      "Epoch 00059: loss did not improve from 7.15205\n",
      "Epoch 60/220\n",
      "\n",
      "Epoch 00060: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.1930 - val_loss: 6.7288\n",
      "\n",
      "Epoch 00060: loss did not improve from 7.15205\n",
      "Epoch 61/220\n",
      "\n",
      "Epoch 00061: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 268s 248ms/step - loss: 7.1882 - val_loss: 6.6873\n",
      "\n",
      "Epoch 00061: loss did not improve from 7.15205\n",
      "Epoch 62/220\n",
      "\n",
      "Epoch 00062: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.1989 - val_loss: 6.7276\n",
      "\n",
      "Epoch 00062: loss did not improve from 7.15205\n",
      "Epoch 63/220\n",
      "\n",
      "Epoch 00063: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 268s 248ms/step - loss: 7.1790 - val_loss: 6.7134\n",
      "\n",
      "Epoch 00063: loss did not improve from 7.15205\n",
      "Epoch 64/220\n",
      "\n",
      "Epoch 00064: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.2245 - val_loss: 6.7387\n",
      "\n",
      "Epoch 00064: loss did not improve from 7.15205\n",
      "Epoch 65/220\n",
      "\n",
      "Epoch 00065: LearningRateScheduler setting learning rate to 0.001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1079/1079 [==============================] - 268s 248ms/step - loss: 7.1926 - val_loss: 6.7223\n",
      "\n",
      "Epoch 00065: loss did not improve from 7.15205\n",
      "Epoch 66/220\n",
      "\n",
      "Epoch 00066: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 247ms/step - loss: 7.1965 - val_loss: 6.7477\n",
      "\n",
      "Epoch 00066: loss did not improve from 7.15205\n",
      "Epoch 67/220\n",
      "\n",
      "Epoch 00067: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.1893 - val_loss: 6.7645\n",
      "\n",
      "Epoch 00067: loss did not improve from 7.15205\n",
      "Epoch 68/220\n",
      "\n",
      "Epoch 00068: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.1857 - val_loss: 6.7326\n",
      "\n",
      "Epoch 00068: loss did not improve from 7.15205\n",
      "Epoch 69/220\n",
      "\n",
      "Epoch 00069: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.2002 - val_loss: 6.7965\n",
      "\n",
      "Epoch 00069: loss did not improve from 7.15205\n",
      "Epoch 70/220\n",
      "\n",
      "Epoch 00070: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.1955 - val_loss: 6.7664\n",
      "\n",
      "Epoch 00070: loss did not improve from 7.15205\n",
      "Epoch 71/220\n",
      "\n",
      "Epoch 00071: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 247ms/step - loss: 7.1851 - val_loss: 6.7193\n",
      "\n",
      "Epoch 00071: loss did not improve from 7.15205\n",
      "Epoch 72/220\n",
      "\n",
      "Epoch 00072: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 247ms/step - loss: 7.2042 - val_loss: 6.7860\n",
      "\n",
      "Epoch 00072: loss did not improve from 7.15205\n",
      "Epoch 73/220\n",
      "\n",
      "Epoch 00073: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.1764 - val_loss: 6.7209\n",
      "\n",
      "Epoch 00073: loss did not improve from 7.15205\n",
      "Epoch 74/220\n",
      "\n",
      "Epoch 00074: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.2008 - val_loss: 6.7737\n",
      "\n",
      "Epoch 00074: loss did not improve from 7.15205\n",
      "Epoch 75/220\n",
      "\n",
      "Epoch 00075: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 247ms/step - loss: 7.1817 - val_loss: 6.7724\n",
      "\n",
      "Epoch 00075: loss did not improve from 7.15205\n",
      "Epoch 76/220\n",
      "\n",
      "Epoch 00076: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.1935 - val_loss: 6.7639\n",
      "\n",
      "Epoch 00076: loss did not improve from 7.15205\n",
      "Epoch 77/220\n",
      "\n",
      "Epoch 00077: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.1886 - val_loss: 6.7866\n",
      "\n",
      "Epoch 00077: loss did not improve from 7.15205\n",
      "Epoch 78/220\n",
      "\n",
      "Epoch 00078: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.1605 - val_loss: 6.7096\n",
      "\n",
      "Epoch 00078: loss did not improve from 7.15205\n",
      "Epoch 79/220\n",
      "\n",
      "Epoch 00079: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.1966 - val_loss: 6.7822\n",
      "\n",
      "Epoch 00079: loss did not improve from 7.15205\n",
      "Epoch 80/220\n",
      "\n",
      "Epoch 00080: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.1834 - val_loss: 6.7842\n",
      "\n",
      "Epoch 00080: loss did not improve from 7.15205\n",
      "Epoch 81/220\n",
      "\n",
      "Epoch 00081: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.1778 - val_loss: 6.7238\n",
      "\n",
      "Epoch 00081: loss did not improve from 7.15205\n",
      "Epoch 82/220\n",
      "\n",
      "Epoch 00082: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.1825 - val_loss: 6.7329\n",
      "\n",
      "Epoch 00082: loss did not improve from 7.15205\n",
      "Epoch 83/220\n",
      "\n",
      "Epoch 00083: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 268s 248ms/step - loss: 7.1793 - val_loss: 6.7840\n",
      "\n",
      "Epoch 00083: loss did not improve from 7.15205\n",
      "Epoch 84/220\n",
      "\n",
      "Epoch 00084: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.1646 - val_loss: 6.7155\n",
      "\n",
      "Epoch 00084: loss did not improve from 7.15205\n",
      "Epoch 85/220\n",
      "\n",
      "Epoch 00085: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.1704 - val_loss: 6.7302\n",
      "\n",
      "Epoch 00085: loss did not improve from 7.15205\n",
      "Epoch 86/220\n",
      "\n",
      "Epoch 00086: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 268s 248ms/step - loss: 7.1707 - val_loss: 6.7459\n",
      "\n",
      "Epoch 00086: loss did not improve from 7.15205\n",
      "Epoch 87/220\n",
      "\n",
      "Epoch 00087: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.1658 - val_loss: 6.7427\n",
      "\n",
      "Epoch 00087: loss did not improve from 7.15205\n",
      "Epoch 88/220\n",
      "\n",
      "Epoch 00088: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.1646 - val_loss: 6.7904\n",
      "\n",
      "Epoch 00088: loss did not improve from 7.15205\n",
      "Epoch 89/220\n",
      "\n",
      "Epoch 00089: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.1596 - val_loss: 6.8185\n",
      "\n",
      "Epoch 00089: loss did not improve from 7.15205\n",
      "Epoch 90/220\n",
      "\n",
      "Epoch 00090: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.1607 - val_loss: 6.8082\n",
      "\n",
      "Epoch 00090: loss did not improve from 7.15205\n",
      "Epoch 91/220\n",
      "\n",
      "Epoch 00091: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 247ms/step - loss: 7.1488 - val_loss: 6.7515\n",
      "\n",
      "Epoch 00091: loss improved from 7.15205 to 7.14877, saving model to ./log_lstm_test/MobileNetv2_ssdLite_bottlrNeck_epoch-91_loss-7.1488_val_loss-6.7515.h5\n",
      "Epoch 92/220\n",
      "\n",
      "Epoch 00092: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 247ms/step - loss: 7.1596 - val_loss: 6.7313\n",
      "\n",
      "Epoch 00092: loss did not improve from 7.14877\n",
      "Epoch 93/220\n",
      "\n",
      "Epoch 00093: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 247ms/step - loss: 7.1561 - val_loss: 6.7521\n",
      "\n",
      "Epoch 00093: loss did not improve from 7.14877\n",
      "Epoch 94/220\n",
      "\n",
      "Epoch 00094: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.1560 - val_loss: 6.7433\n",
      "\n",
      "Epoch 00094: loss did not improve from 7.14877\n",
      "Epoch 95/220\n",
      "\n",
      "Epoch 00095: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.1585 - val_loss: 6.7528\n",
      "\n",
      "Epoch 00095: loss did not improve from 7.14877\n",
      "Epoch 96/220\n",
      "\n",
      "Epoch 00096: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.1598 - val_loss: 6.8140\n",
      "\n",
      "Epoch 00096: loss did not improve from 7.14877\n",
      "Epoch 97/220\n",
      "\n",
      "Epoch 00097: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 268s 248ms/step - loss: 7.1444 - val_loss: 6.7445\n",
      "\n",
      "Epoch 00097: loss improved from 7.14877 to 7.14438, saving model to ./log_lstm_test/MobileNetv2_ssdLite_bottlrNeck_epoch-97_loss-7.1444_val_loss-6.7445.h5\n",
      "Epoch 98/220\n",
      "\n",
      "Epoch 00098: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 247ms/step - loss: 7.1619 - val_loss: 6.8276\n",
      "\n",
      "Epoch 00098: loss did not improve from 7.14438\n",
      "Epoch 99/220\n",
      "\n",
      "Epoch 00099: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 247ms/step - loss: 7.1423 - val_loss: 6.7450\n",
      "\n",
      "Epoch 00099: loss improved from 7.14438 to 7.14227, saving model to ./log_lstm_test/MobileNetv2_ssdLite_bottlrNeck_epoch-99_loss-7.1423_val_loss-6.7450.h5\n",
      "Epoch 100/220\n",
      "\n",
      "Epoch 00100: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 247ms/step - loss: 7.1509 - val_loss: 6.7307\n",
      "\n",
      "Epoch 00100: loss did not improve from 7.14227\n",
      "Epoch 101/220\n",
      "\n",
      "Epoch 00101: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.1553 - val_loss: 6.7533\n",
      "\n",
      "Epoch 00101: loss did not improve from 7.14227\n",
      "Epoch 102/220\n",
      "\n",
      "Epoch 00102: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.1520 - val_loss: 6.7460\n",
      "\n",
      "Epoch 00102: loss did not improve from 7.14227\n",
      "Epoch 103/220\n",
      "\n",
      "Epoch 00103: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 247ms/step - loss: 7.1568 - val_loss: 6.8155\n",
      "\n",
      "Epoch 00103: loss did not improve from 7.14227\n",
      "Epoch 104/220\n",
      "\n",
      "Epoch 00104: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.1404 - val_loss: 6.7328\n",
      "\n",
      "Epoch 00104: loss improved from 7.14227 to 7.14044, saving model to ./log_lstm_test/MobileNetv2_ssdLite_bottlrNeck_epoch-104_loss-7.1404_val_loss-6.7328.h5\n",
      "Epoch 105/220\n",
      "\n",
      "Epoch 00105: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.1526 - val_loss: 6.7345\n",
      "\n",
      "Epoch 00105: loss did not improve from 7.14044\n",
      "Epoch 106/220\n",
      "\n",
      "Epoch 00106: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.1499 - val_loss: 6.7411\n",
      "\n",
      "Epoch 00106: loss did not improve from 7.14044\n",
      "Epoch 107/220\n",
      "\n",
      "Epoch 00107: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 247ms/step - loss: 7.1494 - val_loss: 6.7479\n",
      "\n",
      "Epoch 00107: loss did not improve from 7.14044\n",
      "Epoch 108/220\n",
      "\n",
      "Epoch 00108: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 267s 248ms/step - loss: 7.1572 - val_loss: 6.8176\n",
      "\n",
      "Epoch 00108: loss did not improve from 7.14044\n",
      "Epoch 109/220\n",
      "\n",
      "Epoch 00109: LearningRateScheduler setting learning rate to 0.001.\n",
      "1079/1079 [==============================] - 268s 248ms/step - loss: 7.1452 - val_loss: 6.8124\n",
      "\n",
      "Epoch 00109: loss did not improve from 7.14044\n",
      "Epoch 110/220\n",
      "\n",
      "Epoch 00110: LearningRateScheduler setting learning rate to 0.001.\n",
      " 754/1079 [===================>..........] - ETA: 1:19 - loss: 7.1445"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c851e037c496>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m                               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                               \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataset_size\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tf13/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf13/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf13/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf13/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf13/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf13/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf13/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "initial_epoch   = 0\n",
    "final_epoch     = 220\n",
    "steps_per_epoch =1079#1035#6000\n",
    "val_dataset_size = 520\n",
    "history = model.fit_generator(generator=train_generator,\n",
    "                              steps_per_epoch=steps_per_epoch,\n",
    "                              epochs=final_epoch,\n",
    "                              callbacks=callbacks,\n",
    "                              validation_data=val_generator,\n",
    "                              validation_steps=ceil(val_dataset_size/batch_size),\n",
    "                              initial_epoch=initial_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_generator = val_dataset.generate(batch_size=2,\n",
    "                                         shuffle=True,\n",
    "                                         transformations=[convert_to_3_channels,\n",
    "                                                          resize],\n",
    "                                         label_encoder=None,\n",
    "                                         returns={'processed_images',\n",
    "                                                  'filenames',\n",
    "                                                  'inverse_transform',\n",
    "                                                  'original_images',\n",
    "                                                  'original_labels'},\n",
    "                                         keep_images_without_gt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2: Generate samples.\n",
    "\n",
    "batch_images, batch_filenames, batch_inverse_transforms, batch_original_images, batch_original_labels = next(predict_generator)\n",
    "\n",
    "i = 0 # Which batch item to look at\n",
    "\n",
    "print(\"Image:\", batch_filenames[i])\n",
    "print()\n",
    "print(\"Ground truth boxes:\\n\")\n",
    "print(np.array(batch_original_labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(batch_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4: Decode the raw predictions in `y_pred`.\n",
    "\n",
    "y_pred_decoded = decode_detections(y_pred,\n",
    "                                   confidence_thresh=0.5,\n",
    "                                   iou_threshold=0.4,\n",
    "                                   top_k=200,\n",
    "                                   normalize_coords=normalize_coords,\n",
    "                                   img_height=img_height,\n",
    "                                   img_width=img_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5: Convert the predictions for the original image.\n",
    "\n",
    "y_pred_decoded_inv = apply_inverse_transforms(y_pred_decoded, batch_inverse_transforms)\n",
    "\n",
    "np.set_printoptions(precision=2, suppress=True, linewidth=90)\n",
    "print(\"Predicted boxes:\\n\")\n",
    "print('   class   conf xmin   ymin   xmax   ymax')\n",
    "print(y_pred_decoded_inv[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5: Draw the predicted boxes onto the image\n",
    "\n",
    "# Set the colors for the bounding boxes\n",
    "colors = plt.cm.hsv(np.linspace(0, 1, n_classes+1)).tolist()\n",
    "classes = ['background','human','bicycle','truck','car','bus']\n",
    "\n",
    "plt.figure(figsize=(20,12))\n",
    "plt.imshow(batch_original_images[i])\n",
    "\n",
    "current_axis = plt.gca()\n",
    "\n",
    "for box in batch_original_labels[i]:\n",
    "    xmin = box[1]\n",
    "    ymin = box[2]\n",
    "    xmax = box[3]\n",
    "    ymax = box[4]\n",
    "    label = '{}'.format(classes[int(box[0])])\n",
    "    current_axis.add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color='green', fill=False, linewidth=2))  \n",
    "    current_axis.text(xmin, ymin, label, size='x-large', color='white', bbox={'facecolor':'green', 'alpha':1.0})\n",
    "\n",
    "for box in y_pred_decoded_inv[i]:\n",
    "    xmin = box[2]\n",
    "    ymin = box[3]\n",
    "    xmax = box[4]\n",
    "    ymax = box[5]\n",
    "    color = colors[int(box[0])]\n",
    "    label = '{}: {:.2f}'.format(classes[int(box[0])], box[1])\n",
    "    current_axis.add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color=color, fill=False, linewidth=2))  \n",
    "    current_axis.text(xmin, ymin, label, size='x-large', color='white', bbox={'facecolor':color, 'alpha':1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "192.168.43.211_1596962113120\n",
    "192.168.43.211_1596961348165"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
