{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed value\n",
    "# Apparently you may use different seed values at each stage\n",
    "seed= 1\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "np.random.seed(seed) # seed是一个固定的整数即可\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/xiaoxiao/anaconda3/envs/tf13/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xiaoxiao/anaconda3/envs/tf13/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xiaoxiao/anaconda3/envs/tf13/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:102: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "K.clear_session()\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "#import numpy as np\n",
    "\n",
    "if('tensorflow' == K.backend()):\n",
    "    import tensorflow as tf\n",
    "    from keras.backend.tensorflow_backend import set_session\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "\n",
    "from math import ceil\n",
    "from models.keras_mobilenet_v2_ssdlite_subtraction_allFeatureMaps import mobilenet_v2_ssd\n",
    "#from models.keras_mobilenet_v2_ssdlite_subtraction import mobilenet_v2_ssd\n",
    "\n",
    "from losses.keras_ssd_loss import SSDLoss\n",
    "from data_generatornowinuse.object_detection_2d_data_generator_lstm import DataGenerator\n",
    "from data_generatornowinuse.object_detection_2d_geometric_ops import Resize\n",
    "from data_generatornowinuse.object_detection_2d_photometric_ops import ConvertTo3Channels\n",
    "from data_generatornowinuse.data_augmentation_chain_original_ssd import SSDDataAugmentation\n",
    "from utils.ssd_input_encoder import SSDInputEncoder\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, LearningRateScheduler, TerminateOnNaN, CSVLogger\n",
    "from matplotlib import pyplot as plt\n",
    "from ssd_encoder_decoder.ssd_output_decoder import decode_detections, decode_detections_fast\n",
    "from data_generator.object_detection_2d_misc_utils import apply_inverse_transforms\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SSD configuation\n",
    "\n",
    "img_height = 300 # Height of the model input images\n",
    "img_width = 300 # Width of the model input images\n",
    "img_channels = 3 # Number of color channels of the model input images\n",
    "#mean_color = [123, 117, 104] # The per-channel mean of the images in the dataset. Do not change this value if you're using any of the pre-trained weights.\n",
    "#swap_channels = [2, 1, 0] # The color channel order in the original SSD is BGR, so we'll have the model reverse the color channel order of the input images.\n",
    "n_classes = 7 # Number of positive classes, e.g. 20 for Pascal VOC, 80 for MS COCO\n",
    "\n",
    "#??what decide the anchor box scaling factor??\n",
    "#scale，对于每个featuremap 它的anchor的计算 Sk = Smin +[(Smax - Smin)/(m-1)]*(k-1)\n",
    "#其中Smin默认是0.2,表示最低层的scale为0.2,默认Smax 为0.9,同时也拥有长宽比alpha，所以能求得每个anchor的宽Sk*sqr(alpha)和高Sk/sqr(alpha)\n",
    "#默认 m=6 ， scale:[0.2,0.34,0.48,0.62,0.76,0.9]\n",
    "#结果乘以图片实际款高即可得到anchor的实际大小\n",
    "scales_pascal = [0.1, 0.2, 0.37, 0.54, 0.71, 0.88, 1.05] # min 0.1 max 1.05 The anchor box scaling factors used in the original SSD300 for the Pascal VOC datasets\n",
    "scales_coco = [0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05] # The anchor box scaling factors used in the original SSD300 for the MS COCO datasets\n",
    "scales = scales_pascal\n",
    "#长宽比# 4 6 6 6 4 4\n",
    "aspect_ratios = [[1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5]] # The anchor box aspect ratios used in the original SSD300; the order matters\n",
    "\n",
    "# ??What's the meaning of the factor below??\n",
    "two_boxes_for_ar1 = True\n",
    "steps = [8, 16, 32, 64, 100, 300] # 特征图cell的大小The space between two adjacent anchor box center points for each predictor layer.\n",
    "offsets = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5] # 偏移值，用来确定先验框中心The offsets of the first anchor box center points from the top and left borders of the image as a fraction of the step size for each predictor layer.\n",
    "clip_boxes = False # Whether or not to clip the anchor boxes to lie entirely within the image boundaries\n",
    "#?? Didn't figure out the meaning of the param below\n",
    "variances = [0.1, 0.1, 0.2, 0.2] # The variances by which the encoded target coordinates are divided as in the original implementation\n",
    "normalize_coords = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/xiaoxiao/anaconda3/envs/tf13/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xiaoxiao/anaconda3/envs/tf13/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xiaoxiao/anaconda3/envs/tf13/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xiaoxiao/anaconda3/envs/tf13/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xiaoxiao/anaconda3/envs/tf13/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xiaoxiao/anaconda3/envs/tf13/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xiaoxiao/anaconda3/envs/tf13/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xiaoxiao/anaconda3/envs/tf13/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xiaoxiao/ssdkeras/project_in_git/MobileNetv2_SSDlite/losses/keras_ssd_loss.py:87: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xiaoxiao/ssdkeras/project_in_git/MobileNetv2_SSDlite/losses/keras_ssd_loss.py:129: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From /home/xiaoxiao/ssdkeras/project_in_git/MobileNetv2_SSDlite/losses/keras_ssd_loss.py:68: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/xiaoxiao/ssdkeras/project_in_git/MobileNetv2_SSDlite/losses/keras_ssd_loss.py:170: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "img_input (InputLayer)          (None, 300, 300, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "identity_layer (Lambda)         (None, 300, 300, 3)  0           img_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage1_block1_pad (ZeroPadd (None, 301, 301, 3)  0           identity_layer[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage1_block1_conv (Conv2D) (None, 150, 150, 32) 864         bbn_stage1_block1_pad[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage1_block1_bn (BatchNorm (None, 150, 150, 32) 128         bbn_stage1_block1_conv[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage1_block1_relu (ReLU)   (None, 150, 150, 32) 0           bbn_stage1_block1_bn[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage1_block2_dw_conv (Dept (None, 150, 150, 32) 288         bbn_stage1_block1_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage1_block2_dw_bn (BatchN (None, 150, 150, 32) 128         bbn_stage1_block2_dw_conv[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage1_block2_dw_relu (ReLU (None, 150, 150, 32) 0           bbn_stage1_block2_dw_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage1_block2_project_conv  (None, 150, 150, 16) 512         bbn_stage1_block2_dw_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage1_block2_project_bn (B (None, 150, 150, 16) 64          bbn_stage1_block2_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage2_block1_expand_conv ( (None, 150, 150, 96) 1536        bbn_stage1_block2_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage2_block1_expand_bn (Ba (None, 150, 150, 96) 384         bbn_stage2_block1_expand_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage2_block1_expand_relu ( (None, 150, 150, 96) 0           bbn_stage2_block1_expand_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage2_block1_dw_pad (ZeroP (None, 151, 151, 96) 0           bbn_stage2_block1_expand_relu[0][\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage2_block1_dw_conv (Dept (None, 75, 75, 96)   864         bbn_stage2_block1_dw_pad[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage2_block1_dw_bn (BatchN (None, 75, 75, 96)   384         bbn_stage2_block1_dw_conv[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage2_block1_dw_relu (ReLU (None, 75, 75, 96)   0           bbn_stage2_block1_dw_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage2_block1_project_conv  (None, 75, 75, 24)   2304        bbn_stage2_block1_dw_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage2_block1_project_bn (B (None, 75, 75, 24)   96          bbn_stage2_block1_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage2_block2_expand_conv ( (None, 75, 75, 144)  3456        bbn_stage2_block1_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "sub_input (InputLayer)          (None, 300, 300, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage2_block2_expand_bn (Ba (None, 75, 75, 144)  576         bbn_stage2_block2_expand_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "identity_layer2 (Lambda)        (None, 300, 300, 3)  0           sub_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage2_block2_expand_relu ( (None, 75, 75, 144)  0           bbn_stage2_block2_expand_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "sub_stage1_block1_pad (ZeroPadd (None, 301, 301, 3)  0           identity_layer2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage2_block2_dw_conv (Dept (None, 75, 75, 144)  1296        bbn_stage2_block2_expand_relu[0][\n",
      "__________________________________________________________________________________________________\n",
      "sub_stage1_block1_conv (Conv2D) (None, 150, 150, 32) 864         sub_stage1_block1_pad[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage2_block2_dw_bn (BatchN (None, 75, 75, 144)  576         bbn_stage2_block2_dw_conv[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "sub_stage1_block1_bn (BatchNorm (None, 150, 150, 32) 128         sub_stage1_block1_conv[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage2_block2_dw_relu (ReLU (None, 75, 75, 144)  0           bbn_stage2_block2_dw_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "sub_stage1_block1_relu (ReLU)   (None, 150, 150, 32) 0           sub_stage1_block1_bn[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage2_block2_project_conv  (None, 75, 75, 24)   3456        bbn_stage2_block2_dw_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "sub_stage2_block1_dw_pad (ZeroP (None, 151, 151, 32) 0           sub_stage1_block1_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage2_block2_project_bn (B (None, 75, 75, 24)   96          bbn_stage2_block2_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "sub_stage2_block1_dw_conv (Dept (None, 75, 75, 32)   288         sub_stage2_block1_dw_pad[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage2_block2_add (Add)     (None, 75, 75, 24)   0           bbn_stage2_block1_project_bn[0][0\n",
      "                                                                 bbn_stage2_block2_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "sub_stage2_block1_dw_bn (BatchN (None, 75, 75, 32)   128         sub_stage2_block1_dw_conv[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage3_block1_expand_conv ( (None, 75, 75, 144)  3456        bbn_stage2_block2_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sub_stage2_block1_dw_relu (ReLU (None, 75, 75, 32)   0           sub_stage2_block1_dw_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage3_block1_expand_bn (Ba (None, 75, 75, 144)  576         bbn_stage3_block1_expand_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "sub_stage2_block1_project_conv  (None, 75, 75, 24)   768         sub_stage2_block1_dw_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage3_block1_expand_relu ( (None, 75, 75, 144)  0           bbn_stage3_block1_expand_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "sub_stage2_block1_project_bn (B (None, 75, 75, 24)   96          sub_stage2_block1_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage3_block1_dw_pad (ZeroP (None, 77, 77, 144)  0           bbn_stage3_block1_expand_relu[0][\n",
      "__________________________________________________________________________________________________\n",
      "sub_stage3_block1_dw_pad (ZeroP (None, 77, 77, 24)   0           sub_stage2_block1_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage3_block1_dw_conv (Dept (None, 38, 38, 144)  1296        bbn_stage3_block1_dw_pad[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "sub_stage3_block1_dw_conv (Dept (None, 38, 38, 24)   216         sub_stage3_block1_dw_pad[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage3_block1_dw_bn (BatchN (None, 38, 38, 144)  576         bbn_stage3_block1_dw_conv[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "sub_stage3_block1_dw_bn (BatchN (None, 38, 38, 24)   96          sub_stage3_block1_dw_conv[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage3_block1_dw_relu (ReLU (None, 38, 38, 144)  0           bbn_stage3_block1_dw_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "sub_stage3_block1_dw_relu (ReLU (None, 38, 38, 24)   0           sub_stage3_block1_dw_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage3_block1_project_conv  (None, 38, 38, 32)   4608        bbn_stage3_block1_dw_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "sub_stage3_block1_project_conv  (None, 38, 38, 32)   768         sub_stage3_block1_dw_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage3_block1_project_bn (B (None, 38, 38, 32)   128         bbn_stage3_block1_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "sub_stage3_block1_project_bn (B (None, 38, 38, 32)   128         sub_stage3_block1_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 38, 38, 64)   0           bbn_stage3_block1_project_bn[0][0\n",
      "                                                                 sub_stage3_block1_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage3_block2_expand_conv ( (None, 38, 38, 384)  24576       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage3_block2_expand_bn (Ba (None, 38, 38, 384)  1536        bbn_stage3_block2_expand_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage3_block2_expand_relu ( (None, 38, 38, 384)  0           bbn_stage3_block2_expand_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage3_block2_dw_conv (Dept (None, 38, 38, 384)  3456        bbn_stage3_block2_expand_relu[0][\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage3_block2_dw_bn (BatchN (None, 38, 38, 384)  1536        bbn_stage3_block2_dw_conv[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage3_block2_dw_relu (ReLU (None, 38, 38, 384)  0           bbn_stage3_block2_dw_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage3_block2_project_conv  (None, 38, 38, 32)   12288       bbn_stage3_block2_dw_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage3_block2_project_bn (B (None, 38, 38, 32)   128         bbn_stage3_block2_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage3_block3_expand_conv ( (None, 38, 38, 192)  6144        bbn_stage3_block2_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage3_block3_expand_bn (Ba (None, 38, 38, 192)  768         bbn_stage3_block3_expand_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage3_block3_expand_relu ( (None, 38, 38, 192)  0           bbn_stage3_block3_expand_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage3_block3_dw_conv (Dept (None, 38, 38, 192)  1728        bbn_stage3_block3_expand_relu[0][\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage3_block3_dw_bn (BatchN (None, 38, 38, 192)  768         bbn_stage3_block3_dw_conv[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage3_block3_dw_relu (ReLU (None, 38, 38, 192)  0           bbn_stage3_block3_dw_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage3_block3_project_conv  (None, 38, 38, 32)   6144        bbn_stage3_block3_dw_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage3_block3_project_bn (B (None, 38, 38, 32)   128         bbn_stage3_block3_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage3_block3_add (Add)     (None, 38, 38, 32)   0           bbn_stage3_block2_project_bn[0][0\n",
      "                                                                 bbn_stage3_block3_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block1_expand_conv ( (None, 38, 38, 192)  6144        bbn_stage3_block3_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block1_expand_bn (Ba (None, 38, 38, 192)  768         bbn_stage4_block1_expand_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block1_expand_relu ( (None, 38, 38, 192)  0           bbn_stage4_block1_expand_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block1_dw_pad (ZeroP (None, 39, 39, 192)  0           bbn_stage4_block1_expand_relu[0][\n",
      "__________________________________________________________________________________________________\n",
      "sub_stage4_block1_dw_pad (ZeroP (None, 39, 39, 32)   0           sub_stage3_block1_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block1_dw_conv (Dept (None, 19, 19, 192)  1728        bbn_stage4_block1_dw_pad[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "sub_stage4_block1_dw_conv (Dept (None, 19, 19, 32)   288         sub_stage4_block1_dw_pad[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block1_dw_bn (BatchN (None, 19, 19, 192)  768         bbn_stage4_block1_dw_conv[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "sub_stage4_block1_dw_bn (BatchN (None, 19, 19, 32)   128         sub_stage4_block1_dw_conv[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block1_dw_relu (ReLU (None, 19, 19, 192)  0           bbn_stage4_block1_dw_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "sub_stage4_block1_dw_relu (ReLU (None, 19, 19, 32)   0           sub_stage4_block1_dw_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block1_project_conv  (None, 19, 19, 64)   12288       bbn_stage4_block1_dw_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "sub_stage4_block1_project_conv  (None, 19, 19, 64)   2048        sub_stage4_block1_dw_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block1_project_bn (B (None, 19, 19, 64)   256         bbn_stage4_block1_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "sub_stage4_block1_project_bn (B (None, 19, 19, 64)   256         sub_stage4_block1_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 19, 19, 128)  0           bbn_stage4_block1_project_bn[0][0\n",
      "                                                                 sub_stage4_block1_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block2_expand_conv ( (None, 19, 19, 768)  98304       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block2_expand_bn (Ba (None, 19, 19, 768)  3072        bbn_stage4_block2_expand_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block2_expand_relu ( (None, 19, 19, 768)  0           bbn_stage4_block2_expand_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block2_dw_conv (Dept (None, 19, 19, 768)  6912        bbn_stage4_block2_expand_relu[0][\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block2_dw_bn (BatchN (None, 19, 19, 768)  3072        bbn_stage4_block2_dw_conv[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block2_dw_relu (ReLU (None, 19, 19, 768)  0           bbn_stage4_block2_dw_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block2_project_conv  (None, 19, 19, 64)   49152       bbn_stage4_block2_dw_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block2_project_bn (B (None, 19, 19, 64)   256         bbn_stage4_block2_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block3_expand_conv ( (None, 19, 19, 384)  24576       bbn_stage4_block2_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block3_expand_bn (Ba (None, 19, 19, 384)  1536        bbn_stage4_block3_expand_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block3_expand_relu ( (None, 19, 19, 384)  0           bbn_stage4_block3_expand_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block3_dw_conv (Dept (None, 19, 19, 384)  3456        bbn_stage4_block3_expand_relu[0][\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block3_dw_bn (BatchN (None, 19, 19, 384)  1536        bbn_stage4_block3_dw_conv[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block3_dw_relu (ReLU (None, 19, 19, 384)  0           bbn_stage4_block3_dw_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block3_project_conv  (None, 19, 19, 64)   24576       bbn_stage4_block3_dw_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block3_project_bn (B (None, 19, 19, 64)   256         bbn_stage4_block3_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block3_add (Add)     (None, 19, 19, 64)   0           bbn_stage4_block2_project_bn[0][0\n",
      "                                                                 bbn_stage4_block3_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block4_expand_conv ( (None, 19, 19, 384)  24576       bbn_stage4_block3_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block4_expand_bn (Ba (None, 19, 19, 384)  1536        bbn_stage4_block4_expand_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block4_expand_relu ( (None, 19, 19, 384)  0           bbn_stage4_block4_expand_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block4_dw_conv (Dept (None, 19, 19, 384)  3456        bbn_stage4_block4_expand_relu[0][\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block4_dw_bn (BatchN (None, 19, 19, 384)  1536        bbn_stage4_block4_dw_conv[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block4_dw_relu (ReLU (None, 19, 19, 384)  0           bbn_stage4_block4_dw_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block4_project_conv  (None, 19, 19, 64)   24576       bbn_stage4_block4_dw_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block4_project_bn (B (None, 19, 19, 64)   256         bbn_stage4_block4_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block4_add (Add)     (None, 19, 19, 64)   0           bbn_stage4_block3_add[0][0]      \n",
      "                                                                 bbn_stage4_block4_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block5_expand_conv ( (None, 19, 19, 384)  24576       bbn_stage4_block4_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block5_expand_bn (Ba (None, 19, 19, 384)  1536        bbn_stage4_block5_expand_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block5_expand_relu ( (None, 19, 19, 384)  0           bbn_stage4_block5_expand_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block5_dw_conv (Dept (None, 19, 19, 384)  3456        bbn_stage4_block5_expand_relu[0][\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block5_dw_bn (BatchN (None, 19, 19, 384)  1536        bbn_stage4_block5_dw_conv[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block5_dw_relu (ReLU (None, 19, 19, 384)  0           bbn_stage4_block5_dw_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block5_project_conv  (None, 19, 19, 96)   36864       bbn_stage4_block5_dw_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block5_project_bn (B (None, 19, 19, 96)   384         bbn_stage4_block5_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block6_expand_conv ( (None, 19, 19, 576)  55296       bbn_stage4_block5_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block6_expand_bn (Ba (None, 19, 19, 576)  2304        bbn_stage4_block6_expand_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block6_expand_relu ( (None, 19, 19, 576)  0           bbn_stage4_block6_expand_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block6_dw_conv (Dept (None, 19, 19, 576)  5184        bbn_stage4_block6_expand_relu[0][\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block6_dw_bn (BatchN (None, 19, 19, 576)  2304        bbn_stage4_block6_dw_conv[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block6_dw_relu (ReLU (None, 19, 19, 576)  0           bbn_stage4_block6_dw_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block6_project_conv  (None, 19, 19, 96)   55296       bbn_stage4_block6_dw_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block6_project_bn (B (None, 19, 19, 96)   384         bbn_stage4_block6_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block6_add (Add)     (None, 19, 19, 96)   0           bbn_stage4_block5_project_bn[0][0\n",
      "                                                                 bbn_stage4_block6_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block7_expand_conv ( (None, 19, 19, 576)  55296       bbn_stage4_block6_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block7_expand_bn (Ba (None, 19, 19, 576)  2304        bbn_stage4_block7_expand_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block7_expand_relu ( (None, 19, 19, 576)  0           bbn_stage4_block7_expand_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block7_dw_conv (Dept (None, 19, 19, 576)  5184        bbn_stage4_block7_expand_relu[0][\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block7_dw_bn (BatchN (None, 19, 19, 576)  2304        bbn_stage4_block7_dw_conv[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block7_dw_relu (ReLU (None, 19, 19, 576)  0           bbn_stage4_block7_dw_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block7_project_conv  (None, 19, 19, 96)   55296       bbn_stage4_block7_dw_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block7_project_bn (B (None, 19, 19, 96)   384         bbn_stage4_block7_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage4_block7_add (Add)     (None, 19, 19, 96)   0           bbn_stage4_block6_add[0][0]      \n",
      "                                                                 bbn_stage4_block7_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage5_block1_expand_conv ( (None, 19, 19, 576)  55296       bbn_stage4_block7_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage5_block1_expand_bn (Ba (None, 19, 19, 576)  2304        bbn_stage5_block1_expand_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage5_block1_expand_relu ( (None, 19, 19, 576)  0           bbn_stage5_block1_expand_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage5_block1_dw_pad (ZeroP (None, 21, 21, 576)  0           bbn_stage5_block1_expand_relu[0][\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage5_block1_dw_conv (Dept (None, 10, 10, 576)  5184        bbn_stage5_block1_dw_pad[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage5_block1_dw_bn (BatchN (None, 10, 10, 576)  2304        bbn_stage5_block1_dw_conv[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage5_block1_dw_relu (ReLU (None, 10, 10, 576)  0           bbn_stage5_block1_dw_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage5_block1_project_conv  (None, 10, 10, 160)  92160       bbn_stage5_block1_dw_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage5_block1_project_bn (B (None, 10, 10, 160)  640         bbn_stage5_block1_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stagesub_block1_expand_conv (None, 10, 10, 160)  25600       bbn_stage5_block1_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stagesub_block1_expand_bn ( (None, 10, 10, 160)  640         bbn_stagesub_block1_expand_conv[0\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stagesub_block1_expand_relu (None, 10, 10, 160)  0           bbn_stagesub_block1_expand_bn[0][\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stagesub_block1_dw_conv (De (None, 10, 10, 160)  1440        bbn_stagesub_block1_expand_relu[0\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stagesub_block1_dw_bn (Batc (None, 10, 10, 160)  640         bbn_stagesub_block1_dw_conv[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stagesub_block1_dw_relu (Re (None, 10, 10, 160)  0           bbn_stagesub_block1_dw_bn[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stagesub_block1_project_con (None, 10, 10, 160)  25600       bbn_stagesub_block1_dw_relu[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stagesub_block1_project_bn  (None, 10, 10, 160)  640         bbn_stagesub_block1_project_conv[\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stagesub_block1_add (Add)   (None, 10, 10, 160)  0           bbn_stage5_block1_project_bn[0][0\n",
      "                                                                 bbn_stagesub_block1_project_bn[0]\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage5_block2_expand_conv ( (None, 10, 10, 960)  153600      bbn_stagesub_block1_add[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage5_block2_expand_bn (Ba (None, 10, 10, 960)  3840        bbn_stage5_block2_expand_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage5_block2_expand_relu ( (None, 10, 10, 960)  0           bbn_stage5_block2_expand_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage5_block2_dw_conv (Dept (None, 10, 10, 960)  8640        bbn_stage5_block2_expand_relu[0][\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage5_block2_dw_bn (BatchN (None, 10, 10, 960)  3840        bbn_stage5_block2_dw_conv[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage5_block2_dw_relu (ReLU (None, 10, 10, 960)  0           bbn_stage5_block2_dw_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage5_block2_project_conv  (None, 10, 10, 160)  153600      bbn_stage5_block2_dw_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage5_block2_project_bn (B (None, 10, 10, 160)  640         bbn_stage5_block2_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage5_block2_add (Add)     (None, 10, 10, 160)  0           bbn_stagesub_block1_add[0][0]    \n",
      "                                                                 bbn_stage5_block2_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage5_block3_expand_conv ( (None, 10, 10, 960)  153600      bbn_stage5_block2_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage5_block3_expand_bn (Ba (None, 10, 10, 960)  3840        bbn_stage5_block3_expand_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage5_block3_expand_relu ( (None, 10, 10, 960)  0           bbn_stage5_block3_expand_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage5_block3_dw_conv (Dept (None, 10, 10, 960)  8640        bbn_stage5_block3_expand_relu[0][\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage5_block3_dw_bn (BatchN (None, 10, 10, 960)  3840        bbn_stage5_block3_dw_conv[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage5_block3_dw_relu (ReLU (None, 10, 10, 960)  0           bbn_stage5_block3_dw_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage5_block3_project_conv  (None, 10, 10, 160)  153600      bbn_stage5_block3_dw_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage5_block3_project_bn (B (None, 10, 10, 160)  640         bbn_stage5_block3_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage5_block3_add (Add)     (None, 10, 10, 160)  0           bbn_stage5_block2_add[0][0]      \n",
      "                                                                 bbn_stage5_block3_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage5_block4_expand_conv ( (None, 10, 10, 960)  153600      bbn_stage5_block3_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage5_block4_expand_bn (Ba (None, 10, 10, 960)  3840        bbn_stage5_block4_expand_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage5_block4_expand_relu ( (None, 10, 10, 960)  0           bbn_stage5_block4_expand_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage5_block4_dw_conv (Dept (None, 10, 10, 960)  8640        bbn_stage5_block4_expand_relu[0][\n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage5_block4_dw_bn (BatchN (None, 10, 10, 960)  3840        bbn_stage5_block4_dw_conv[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage5_block4_dw_relu (ReLU (None, 10, 10, 960)  0           bbn_stage5_block4_dw_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage5_block4_project_conv  (None, 10, 10, 320)  307200      bbn_stage5_block4_dw_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "bbn_stage5_block4_project_bn (B (None, 10, 10, 320)  1280        bbn_stage5_block4_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "ssd_2_conv (Conv2D)             (None, 10, 10, 1280) 409600      bbn_stage5_block4_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "ssd_2_conv_bn (BatchNormalizati (None, 10, 10, 1280) 5120        ssd_2_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "ssd_2_conv_relu (ReLU)          (None, 10, 10, 1280) 0           ssd_2_conv_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ssd_3_conv (Conv2D)             (None, 10, 10, 256)  327680      ssd_2_conv_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "ssd_3_conv_bn (BatchNormalizati (None, 10, 10, 256)  1024        ssd_3_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "ssd_3_conv_relu (ReLU)          (None, 10, 10, 256)  0           ssd_3_conv_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ssd_3_dw_pad (ZeroPadding2D)    (None, 11, 11, 256)  0           ssd_3_conv_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "ssd_3_dw_conv (DepthwiseConv2D) (None, 5, 5, 256)    2304        ssd_3_dw_pad[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "ssd_3_dw_bn (BatchNormalization (None, 5, 5, 256)    1024        ssd_3_dw_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ssd_3_dw_relu (ReLU)            (None, 5, 5, 256)    0           ssd_3_dw_bn[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "ssd_3_conv2 (Conv2D)            (None, 5, 5, 512)    131072      ssd_3_dw_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ssd_3_conv2_bn (BatchNormalizat (None, 5, 5, 512)    2048        ssd_3_conv2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "ssd_3_conv2_relu (ReLU)         (None, 5, 5, 512)    0           ssd_3_conv2_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "ssd_4_conv (Conv2D)             (None, 5, 5, 128)    65536       ssd_3_conv2_relu[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_4_conv_bn (BatchNormalizati (None, 5, 5, 128)    512         ssd_4_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "ssd_4_conv_relu (ReLU)          (None, 5, 5, 128)    0           ssd_4_conv_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ssd_4_dw_pad (ZeroPadding2D)    (None, 7, 7, 128)    0           ssd_4_conv_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "ssd_4_dw_conv (DepthwiseConv2D) (None, 3, 3, 128)    1152        ssd_4_dw_pad[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "ssd_4_dw_bn (BatchNormalization (None, 3, 3, 128)    512         ssd_4_dw_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ssd_4_dw_relu (ReLU)            (None, 3, 3, 128)    0           ssd_4_dw_bn[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "ssd_4_conv2 (Conv2D)            (None, 3, 3, 256)    32768       ssd_4_dw_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ssd_4_conv2_bn (BatchNormalizat (None, 3, 3, 256)    1024        ssd_4_conv2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "ssd_4_conv2_relu (ReLU)         (None, 3, 3, 256)    0           ssd_4_conv2_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "ssd_5_conv (Conv2D)             (None, 3, 3, 128)    32768       ssd_4_conv2_relu[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_5_conv_bn (BatchNormalizati (None, 3, 3, 128)    512         ssd_5_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "ssd_5_conv_relu (ReLU)          (None, 3, 3, 128)    0           ssd_5_conv_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ssd_5_dw_pad (ZeroPadding2D)    (None, 5, 5, 128)    0           ssd_5_conv_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "ssd_5_dw_conv (DepthwiseConv2D) (None, 2, 2, 128)    1152        ssd_5_dw_pad[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "ssd_5_dw_bn (BatchNormalization (None, 2, 2, 128)    512         ssd_5_dw_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ssd_5_dw_relu (ReLU)            (None, 2, 2, 128)    0           ssd_5_dw_bn[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "ssd_5_conv2 (Conv2D)            (None, 2, 2, 256)    32768       ssd_5_dw_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ssd_5_conv2_bn (BatchNormalizat (None, 2, 2, 256)    1024        ssd_5_conv2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "ssd_5_conv2_relu (ReLU)         (None, 2, 2, 256)    0           ssd_5_conv2_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "ssd_6_conv (Conv2D)             (None, 2, 2, 64)     16384       ssd_5_conv2_relu[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_6_conv_bn (BatchNormalizati (None, 2, 2, 64)     256         ssd_6_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "ssd_6_conv_relu (ReLU)          (None, 2, 2, 64)     0           ssd_6_conv_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ssd_6_dw_pad (ZeroPadding2D)    (None, 3, 3, 64)     0           ssd_6_conv_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "ssd_6_dw_conv (DepthwiseConv2D) (None, 1, 1, 64)     576         ssd_6_dw_pad[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "ssd_6_dw_bn (BatchNormalization (None, 1, 1, 64)     256         ssd_6_dw_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ssd_6_dw_relu (ReLU)            (None, 1, 1, 64)     0           ssd_6_dw_bn[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "ssd_6_conv2 (Conv2D)            (None, 1, 1, 128)    8192        ssd_6_dw_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ssd_6_conv2_bn (BatchNormalizat (None, 1, 1, 128)    512         ssd_6_conv2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "ssd_6_conv2_relu (ReLU)         (None, 1, 1, 128)    0           ssd_6_conv2_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "ssd_cls1_dw_conv (DepthwiseConv (None, 19, 19, 576)  5184        bbn_stage5_block1_expand_relu[0][\n",
      "__________________________________________________________________________________________________\n",
      "ssd_cls2_dw_conv (DepthwiseConv (None, 10, 10, 1280) 11520       ssd_2_conv_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "ssd_cls3_dw_conv (DepthwiseConv (None, 5, 5, 512)    4608        ssd_3_conv2_relu[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_cls4_dw_conv (DepthwiseConv (None, 3, 3, 256)    2304        ssd_4_conv2_relu[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_cls5_dw_conv (DepthwiseConv (None, 2, 2, 256)    2304        ssd_5_conv2_relu[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_cls6_dw_conv (DepthwiseConv (None, 1, 1, 128)    1152        ssd_6_conv2_relu[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_box1_dw_conv (DepthwiseConv (None, 19, 19, 576)  5184        bbn_stage5_block1_expand_relu[0][\n",
      "__________________________________________________________________________________________________\n",
      "ssd_box2_dw_conv (DepthwiseConv (None, 10, 10, 1280) 11520       ssd_2_conv_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "ssd_box3_dw_conv (DepthwiseConv (None, 5, 5, 512)    4608        ssd_3_conv2_relu[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_box4_dw_conv (DepthwiseConv (None, 3, 3, 256)    2304        ssd_4_conv2_relu[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_box5_dw_conv (DepthwiseConv (None, 2, 2, 256)    2304        ssd_5_conv2_relu[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_box6_dw_conv (DepthwiseConv (None, 1, 1, 128)    1152        ssd_6_conv2_relu[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_cls1_dw_bn (BatchNormalizat (None, 19, 19, 576)  2304        ssd_cls1_dw_conv[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_cls2_dw_bn (BatchNormalizat (None, 10, 10, 1280) 5120        ssd_cls2_dw_conv[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_cls3_dw_bn (BatchNormalizat (None, 5, 5, 512)    2048        ssd_cls3_dw_conv[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_cls4_dw_bn (BatchNormalizat (None, 3, 3, 256)    1024        ssd_cls4_dw_conv[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_cls5_dw_bn (BatchNormalizat (None, 2, 2, 256)    1024        ssd_cls5_dw_conv[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_cls6_dw_bn (BatchNormalizat (None, 1, 1, 128)    512         ssd_cls6_dw_conv[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_box1_dw_bn (BatchNormalizat (None, 19, 19, 576)  2304        ssd_box1_dw_conv[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_box2_dw_bn (BatchNormalizat (None, 10, 10, 1280) 5120        ssd_box2_dw_conv[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_box3_dw_bn (BatchNormalizat (None, 5, 5, 512)    2048        ssd_box3_dw_conv[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_box4_dw_bn (BatchNormalizat (None, 3, 3, 256)    1024        ssd_box4_dw_conv[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_box5_dw_bn (BatchNormalizat (None, 2, 2, 256)    1024        ssd_box5_dw_conv[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_box6_dw_bn (BatchNormalizat (None, 1, 1, 128)    512         ssd_box6_dw_conv[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_cls1_dw_relu (ReLU)         (None, 19, 19, 576)  0           ssd_cls1_dw_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "ssd_cls2_dw_relu (ReLU)         (None, 10, 10, 1280) 0           ssd_cls2_dw_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "ssd_cls3_dw_relu (ReLU)         (None, 5, 5, 512)    0           ssd_cls3_dw_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "ssd_cls4_dw_relu (ReLU)         (None, 3, 3, 256)    0           ssd_cls4_dw_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "ssd_cls5_dw_relu (ReLU)         (None, 2, 2, 256)    0           ssd_cls5_dw_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "ssd_cls6_dw_relu (ReLU)         (None, 1, 1, 128)    0           ssd_cls6_dw_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "ssd_box1_dw_relu (ReLU)         (None, 19, 19, 576)  0           ssd_box1_dw_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "ssd_box2_dw_relu (ReLU)         (None, 10, 10, 1280) 0           ssd_box2_dw_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "ssd_box3_dw_relu (ReLU)         (None, 5, 5, 512)    0           ssd_box3_dw_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "ssd_box4_dw_relu (ReLU)         (None, 3, 3, 256)    0           ssd_box4_dw_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "ssd_box5_dw_relu (ReLU)         (None, 2, 2, 256)    0           ssd_box5_dw_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "ssd_box6_dw_relu (ReLU)         (None, 1, 1, 128)    0           ssd_box6_dw_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "ssd_cls1conv2 (Conv2D)          (None, 19, 19, 32)   18432       ssd_cls1_dw_relu[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_cls2conv2 (Conv2D)          (None, 10, 10, 48)   61440       ssd_cls2_dw_relu[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_cls3conv2 (Conv2D)          (None, 5, 5, 48)     24576       ssd_cls3_dw_relu[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_cls4conv2 (Conv2D)          (None, 3, 3, 48)     12288       ssd_cls4_dw_relu[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_cls5conv2 (Conv2D)          (None, 2, 2, 32)     8192        ssd_cls5_dw_relu[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_cls6conv2 (Conv2D)          (None, 1, 1, 32)     4096        ssd_cls6_dw_relu[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_box1conv2 (Conv2D)          (None, 19, 19, 16)   9216        ssd_box1_dw_relu[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_box2conv2 (Conv2D)          (None, 10, 10, 24)   30720       ssd_box2_dw_relu[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_box3conv2 (Conv2D)          (None, 5, 5, 24)     12288       ssd_box3_dw_relu[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_box4conv2 (Conv2D)          (None, 3, 3, 24)     6144        ssd_box4_dw_relu[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_box5conv2 (Conv2D)          (None, 2, 2, 16)     4096        ssd_box5_dw_relu[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_box6conv2 (Conv2D)          (None, 1, 1, 16)     2048        ssd_box6_dw_relu[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_cls1conv2_bn (BatchNormaliz (None, 19, 19, 32)   128         ssd_cls1conv2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ssd_cls2conv2_bn (BatchNormaliz (None, 10, 10, 48)   192         ssd_cls2conv2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ssd_cls3conv2_bn (BatchNormaliz (None, 5, 5, 48)     192         ssd_cls3conv2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ssd_cls4conv2_bn (BatchNormaliz (None, 3, 3, 48)     192         ssd_cls4conv2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ssd_cls5conv2_bn (BatchNormaliz (None, 2, 2, 32)     128         ssd_cls5conv2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ssd_cls6conv2_bn (BatchNormaliz (None, 1, 1, 32)     128         ssd_cls6conv2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ssd_box1conv2_bn (BatchNormaliz (None, 19, 19, 16)   64          ssd_box1conv2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ssd_box2conv2_bn (BatchNormaliz (None, 10, 10, 24)   96          ssd_box2conv2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ssd_box3conv2_bn (BatchNormaliz (None, 5, 5, 24)     96          ssd_box3conv2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ssd_box4conv2_bn (BatchNormaliz (None, 3, 3, 24)     96          ssd_box4conv2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ssd_box5conv2_bn (BatchNormaliz (None, 2, 2, 16)     64          ssd_box5conv2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ssd_box6conv2_bn (BatchNormaliz (None, 1, 1, 16)     64          ssd_box6conv2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ssd_cls1_reshape (Reshape)      (None, 1444, 8)      0           ssd_cls1conv2_bn[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_cls2_reshape (Reshape)      (None, 600, 8)       0           ssd_cls2conv2_bn[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_cls3_reshape (Reshape)      (None, 150, 8)       0           ssd_cls3conv2_bn[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_cls4_reshape (Reshape)      (None, 54, 8)        0           ssd_cls4conv2_bn[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_cls5_reshape (Reshape)      (None, 16, 8)        0           ssd_cls5conv2_bn[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_cls6_reshape (Reshape)      (None, 4, 8)         0           ssd_cls6conv2_bn[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_priorbox_1 (AnchorBoxes)    (None, 19, 19, 4, 8) 0           ssd_box1conv2_bn[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_priorbox_2 (AnchorBoxes)    (None, 10, 10, 6, 8) 0           ssd_box2conv2_bn[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_priorbox_3 (AnchorBoxes)    (None, 5, 5, 6, 8)   0           ssd_box3conv2_bn[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_priorbox_4 (AnchorBoxes)    (None, 3, 3, 6, 8)   0           ssd_box4conv2_bn[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_priorbox_5 (AnchorBoxes)    (None, 2, 2, 4, 8)   0           ssd_box5conv2_bn[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_priorbox_6 (AnchorBoxes)    (None, 1, 1, 4, 8)   0           ssd_box6conv2_bn[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_cls (Concatenate)           (None, 2268, 8)      0           ssd_cls1_reshape[0][0]           \n",
      "                                                                 ssd_cls2_reshape[0][0]           \n",
      "                                                                 ssd_cls3_reshape[0][0]           \n",
      "                                                                 ssd_cls4_reshape[0][0]           \n",
      "                                                                 ssd_cls5_reshape[0][0]           \n",
      "                                                                 ssd_cls6_reshape[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_box1_reshape (Reshape)      (None, 1444, 4)      0           ssd_box1conv2_bn[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_box2_reshape (Reshape)      (None, 600, 4)       0           ssd_box2conv2_bn[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_box3_reshape (Reshape)      (None, 150, 4)       0           ssd_box3conv2_bn[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_box4_reshape (Reshape)      (None, 54, 4)        0           ssd_box4conv2_bn[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_box5_reshape (Reshape)      (None, 16, 4)        0           ssd_box5conv2_bn[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_box6_reshape (Reshape)      (None, 4, 4)         0           ssd_box6conv2_bn[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_priorbox1_reshape (Reshape) (None, 1444, 8)      0           ssd_priorbox_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "ssd_priorbox2_reshape (Reshape) (None, 600, 8)       0           ssd_priorbox_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "ssd_priorbox3_reshape (Reshape) (None, 150, 8)       0           ssd_priorbox_3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "ssd_priorbox4_reshape (Reshape) (None, 54, 8)        0           ssd_priorbox_4[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "ssd_priorbox5_reshape (Reshape) (None, 16, 8)        0           ssd_priorbox_5[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "ssd_priorbox6_reshape (Reshape) (None, 4, 8)         0           ssd_priorbox_6[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "ssd_mbox_conf_softmax (Activati (None, 2268, 8)      0           ssd_cls[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "ssd_box (Concatenate)           (None, 2268, 4)      0           ssd_box1_reshape[0][0]           \n",
      "                                                                 ssd_box2_reshape[0][0]           \n",
      "                                                                 ssd_box3_reshape[0][0]           \n",
      "                                                                 ssd_box4_reshape[0][0]           \n",
      "                                                                 ssd_box5_reshape[0][0]           \n",
      "                                                                 ssd_box6_reshape[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ssd_priorbox (Concatenate)      (None, 2268, 8)      0           ssd_priorbox1_reshape[0][0]      \n",
      "                                                                 ssd_priorbox2_reshape[0][0]      \n",
      "                                                                 ssd_priorbox3_reshape[0][0]      \n",
      "                                                                 ssd_priorbox4_reshape[0][0]      \n",
      "                                                                 ssd_priorbox5_reshape[0][0]      \n",
      "                                                                 ssd_priorbox6_reshape[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "ssd_predictions (Concatenate)   (None, 2268, 20)     0           ssd_mbox_conf_softmax[0][0]      \n",
      "                                                                 ssd_box[0][0]                    \n",
      "                                                                 ssd_priorbox[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 3,386,168\n",
      "Trainable params: 3,330,952\n",
      "Non-trainable params: 55,216\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Build the model\n",
    "K.clear_session()\n",
    "\n",
    "model = mobilenet_v2_ssd(image_size=(img_height, img_width, img_channels),\n",
    "                n_classes=n_classes,\n",
    "                mode='training',\n",
    "                l2_regularization=0.0005,\n",
    "                scales=scales,\n",
    "                aspect_ratios_per_layer=aspect_ratios,\n",
    "                two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                steps=steps,\n",
    "                offsets=offsets,\n",
    "                clip_boxes=clip_boxes,\n",
    "                variances=variances,\n",
    "                normalize_coords=normalize_coords)\n",
    "# train from scratch.0, 2.0, 0.5, 3.0, 1.0/3.0],h, no weights to load\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
    "# set_trainable(r\"(ssd\\_[cls|box].*)\", model)\n",
    "model.compile(optimizer=adam, loss=ssd_loss.compute_loss)\n",
    "\n",
    "print(model.summary())\n",
    "#Total params: 3,160,240\n",
    "#Trainable params: 3,108,888"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image set 'train.txt': 100%|██████████| 12896/12896 [00:13<00:00, 926.78it/s] \n",
      "Processing image set 'val.txt': 100%|██████████| 836/836 [00:00<00:00, 962.17it/s] \n",
      "Creating HDF5 dataset: 100%|██████████| 12896/12896 [03:18<00:00, 64.86it/s] \n",
      "Creating HDF5 dataset: 100%|██████████| 836/836 [00:09<00:00, 89.52it/s] \n"
     ]
    }
   ],
   "source": [
    "# 1: Instantiate two `DataGenerator` objects: One for training, one for validation.\n",
    "\n",
    "# Optional: If you have enough memory, consider loading the images into memory for the reasons explained above.\n",
    "\n",
    "train_dataset = DataGenerator()#(load_images_into_memory=False, hdf5_dataset_path=None)\n",
    "val_dataset = DataGenerator()#(load_images_into_memory=False, hdf5_dataset_path=None)\n",
    "\n",
    "# 2: Parse the image and label lists for the training and validation datasets. This can take a while.\n",
    "\n",
    "# TODO: Set the paths to the datasets here.\n",
    "\n",
    "# The directories that contain the images.\n",
    "VOC_2007_images_dir = './data_index/data/pic/'\n",
    "\n",
    "# The directories that contain the annotations.\n",
    "VOC_2007_annotations_dir = './data_index/data/label/'\n",
    "\n",
    "VOC_2007_trainval_image_set_filename = './data_index/data/train.txt'\n",
    "#VOC_2012_trainval_image_set_filename = '../../datasets/VOCdevkit/VOC2012/ImageSets/Main/trainval.txt'\n",
    "VOC_2007_test_image_set_filename     = './data_index/data/val.txt'\n",
    "\n",
    "# The XML parser needs to now what object class names to look for and in which order to map them to integers.\n",
    "#classes = ['background','human','bicycle','truck','car','bus','motorbike','escooter']\n",
    "classes = ['background','human','bicycle','truck','car','bus','escooter','motorbike']\n",
    "\n",
    "\n",
    "train_dataset.parse_xml(images_dirs=[VOC_2007_images_dir],\n",
    "                        image_set_filenames=[VOC_2007_trainval_image_set_filename],\n",
    "                        annotations_dirs=[VOC_2007_annotations_dir],\n",
    "                        classes=classes,\n",
    "                        include_classes='all',\n",
    "                        exclude_truncated=False,\n",
    "                        exclude_difficult=False,\n",
    "                        ret=False)\n",
    "\n",
    "val_dataset.parse_xml(images_dirs=[VOC_2007_images_dir],\n",
    "                      image_set_filenames=[VOC_2007_test_image_set_filename],\n",
    "                      annotations_dirs=[VOC_2007_annotations_dir],\n",
    "                      classes=classes,\n",
    "                      include_classes='all',\n",
    "                      exclude_truncated=False,\n",
    "                      exclude_difficult=False,#used to be True\n",
    "                      ret=False)\n",
    "\n",
    "# Optional: Convert the dataset into an HDF5 dataset. This will require more disk space, but will\n",
    "# speed up the training. Doing this is not relevant in case you activated the `load_images_into_memory`\n",
    "# option in the constructor, because in that cas the images are in memory already anyway. If you don't\n",
    "# want to create HDF5 datasets, comment out the subsequent two function calls.\n",
    "\n",
    "train_dataset.create_hdf5_dataset(file_path='dataset_pascal_voc_07_trainval.h5',\n",
    "                                  resize=False,\n",
    "                                  variable_image_size=True,\n",
    "                                  verbose=True)\n",
    "\n",
    "val_dataset.create_hdf5_dataset(file_path='dataset_pascal_voc_07_test.h5',\n",
    "                                resize=False,\n",
    "                                variable_image_size=True,\n",
    "                                verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate schedule\n",
    "def lr_schedule(epoch):\n",
    "    if epoch < 200:\n",
    "        return 0.001\n",
    "    elif epoch < 500:\n",
    "        return 0.0001\n",
    "    else:\n",
    "        return 0.00001\n",
    "\n",
    "\n",
    "# set trainable layers\n",
    "def set_trainable(layer_regex, keras_model=None, indent=0, verbose=1):\n",
    "    # In multi-GPU training, we wrap the model. Get layers\n",
    "    # of the inner model because they have the weights.\n",
    "    layers = keras_model.inner_model.layers if hasattr(keras_model, \"inner_model\") \\\n",
    "        else keras_model.layers\n",
    "\n",
    "    for layer in layers:\n",
    "        # Is the layer a model?\n",
    "        if layer.__class__.__name__ == 'Model':\n",
    "            print(\"In model: \", layer.name)\n",
    "            set_trainable(\n",
    "                layer_regex, keras_model=layer)\n",
    "            continue\n",
    "\n",
    "        if not layer.weights:\n",
    "            continue\n",
    "        # Is it trainable?\n",
    "        trainable = bool(re.fullmatch(layer_regex, layer.name))\n",
    "        # Update layer. If layer is a container, update inner layer.\n",
    "        if layer.__class__.__name__ == 'TimeDistributed':\n",
    "            layer.layer.trainable = trainable\n",
    "        else:\n",
    "            layer.trainable = trainable\n",
    "        # Print trainable layer names\n",
    "        if trainable and verbose > 0:\n",
    "            print(\"{}{:20}   ({})\".format(\" \" * indent, layer.name, layer.__class__.__name__))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3: Set the batch size.\n",
    "\n",
    "batch_size = 26#32 # Change the batch size if you like, or if you run into GPU memory issues.\n",
    "\n",
    "# 4: Set the train_datasetimage transformations for pre-processing and data augmentation options.\n",
    "\n",
    "# For the training generator:\n",
    "ssd_data_augmentation = SSDDataAugmentation(img_height=img_height,\n",
    "                                            img_width=img_width)\n",
    "\n",
    "# For the validation generator:\n",
    "convert_to_3_channels = ConvertTo3Channels()\n",
    "resize = Resize(height=img_height, width=img_width)\n",
    "\n",
    "# 5: Instantiate an encoder that can encode ground truth labels into the format needed by the SSD loss function.\n",
    "#用getlayer来获取输出层的尺寸\n",
    "# The encoder constructor needs the spatial dimensions of the model's predictor layers to create the anchor boxes.\n",
    "predictor_sizes = [model.get_layer('ssd_cls1conv2_bn').output_shape[1:3],\n",
    "                   model.get_layer('ssd_cls2conv2_bn').output_shape[1:3],\n",
    "                   model.get_layer('ssd_cls3conv2_bn').output_shape[1:3],\n",
    "                   model.get_layer('ssd_cls4conv2_bn').output_shape[1:3],\n",
    "                   model.get_layer('ssd_cls5conv2_bn').output_shape[1:3],\n",
    "                   model.get_layer('ssd_cls6conv2_bn').output_shape[1:3]]\n",
    "#encoder把ground truth labels\n",
    "ssd_input_encoder = SSDInputEncoder(img_height=img_height,\n",
    "                                    img_width=img_width,\n",
    "                                    n_classes=n_classes,\n",
    "                                    predictor_sizes=predictor_sizes,\n",
    "                                    scales=scales,\n",
    "                                    aspect_ratios_per_layer=aspect_ratios,\n",
    "                                    two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                                    steps=steps,\n",
    "                                    offsets=offsets,\n",
    "                                    clip_boxes=clip_boxes,\n",
    "                                    variances=variances,\n",
    "                                    matching_type='multi',\n",
    "                                    pos_iou_threshold=0.5,\n",
    "                                    neg_iou_limit=0.5,\n",
    "                                    normalize_coords=normalize_coords)\n",
    "\n",
    "# 6: Create the generator handles that will be passed to Keras' `fit_generator()` function.\n",
    "\n",
    "train_generator = train_dataset.generate(batch_size=batch_size,\n",
    "                                         shuffle=True,\n",
    "                                         transformations=[ssd_data_augmentation],# used to be augumentation\n",
    "                                         label_encoder=ssd_input_encoder,\n",
    "                                         returns={'processed_images',\n",
    "                                                  'encoded_labels'},\n",
    "                                         keep_images_without_gt=False)\n",
    "\n",
    "val_generator = val_dataset.generate(batch_size=batch_size,\n",
    "                                     shuffle=False,\n",
    "                                     transformations=[convert_to_3_channels,\n",
    "                                                      resize],\n",
    "                                     label_encoder=ssd_input_encoder,\n",
    "                                     returns={'processed_images',\n",
    "                                              'encoded_labels'},\n",
    "                                     keep_images_without_gt=False)\n",
    "\n",
    "# Get the number of samples in the training and validations datasets.\n",
    "#train_dataset_size = train_dataset.get_dataset_size()\n",
    "#val_dataset_size   = val_dataset.get_dataset_size()\n",
    "\n",
    "#print(\"Number of images in the training dataset:\\t{:>6}\".format(train_dataset_size))\n",
    "#print(\"Number of images in the validation dataset:\\t{:>6}\".format(val_dataset_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfilepath_trainedModel=\\'./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-90_loss-3.3454_val_loss-3.8132.h5\\'\\nif os.path.exists(filepath_trainedModel):\\n    model.load_weights(filepath_trainedModel)\\n    # 若成功加载前面保存的参数，输出下列信息\\n    print(\"checkpoint_loaded\")      '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#断点续练\n",
    "'''\n",
    "filepath_trainedModel='./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-90_loss-3.3454_val_loss-3.8132.h5'\n",
    "if os.path.exists(filepath_trainedModel):\n",
    "    model.load_weights(filepath_trainedModel)\n",
    "    # 若成功加载前面保存的参数，输出下列信息\n",
    "    print(\"checkpoint_loaded\")      '''                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model callbacks.\n",
    "\n",
    "# TODO: Set the filepath under which you want to save the model.\n",
    "model_checkpoint = ModelCheckpoint(filepath='./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-{epoch:02d}_loss-{loss:.4f}_val_loss-{val_loss:.4f}.h5',\n",
    "                                   monitor='loss',#used to be val_loss\n",
    "                                   verbose=1,\n",
    "                                   save_best_only=True,\n",
    "                                   save_weights_only=False,\n",
    "                                   mode='auto',\n",
    "                                   period=1)\n",
    "#model_checkpoint.best = \n",
    "\n",
    "csv_logger = CSVLogger(filename='MobileNetv2_ssdLite_training_concatStage3_4_log_id17.csv',\n",
    "                       separator=',',\n",
    "                       append=True)\n",
    "\n",
    "learning_rate_scheduler = LearningRateScheduler(schedule=lr_schedule,\n",
    "                                                verbose=1)\n",
    "\n",
    "terminate_on_nan = TerminateOnNaN()\n",
    "\n",
    "callbacks = [model_checkpoint,\n",
    "             csv_logger,\n",
    "             learning_rate_scheduler,\n",
    "             terminate_on_nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/xiaoxiao/anaconda3/envs/tf13/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xiaoxiao/anaconda3/envs/tf13/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Epoch 1/120\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 346s 699ms/step - loss: 15.1025 - val_loss: 15.3489\n",
      "\n",
      "Epoch 00001: loss improved from inf to 15.10119, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-01_loss-15.1012_val_loss-15.3489.h5\n",
      "Epoch 2/120\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 317s 640ms/step - loss: 11.2222 - val_loss: 13.2643\n",
      "\n",
      "Epoch 00002: loss improved from 15.10119 to 11.21444, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-02_loss-11.2144_val_loss-13.2643.h5\n",
      "Epoch 3/120\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 323s 650ms/step - loss: 9.7460 - val_loss: 14.5429\n",
      "\n",
      "Epoch 00003: loss improved from 11.21444 to 9.72965, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-03_loss-9.7297_val_loss-14.5429.h5\n",
      "Epoch 4/120\n",
      "\n",
      "Epoch 00004: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 322s 649ms/step - loss: 8.7965 - val_loss: 10.5592\n",
      "\n",
      "Epoch 00004: loss improved from 9.72965 to 8.79641, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-04_loss-8.7964_val_loss-10.5592.h5\n",
      "Epoch 5/120\n",
      "\n",
      "Epoch 00005: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 322s 650ms/step - loss: 8.1230 - val_loss: 10.8672\n",
      "\n",
      "Epoch 00005: loss improved from 8.79641 to 8.11792, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-05_loss-8.1179_val_loss-10.8672.h5\n",
      "Epoch 6/120\n",
      "\n",
      "Epoch 00006: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 324s 653ms/step - loss: 7.6435 - val_loss: 9.6967\n",
      "\n",
      "Epoch 00006: loss improved from 8.11792 to 7.64372, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-06_loss-7.6437_val_loss-9.6967.h5\n",
      "Epoch 7/120\n",
      "\n",
      "Epoch 00007: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 322s 650ms/step - loss: 7.2283 - val_loss: 7.4088\n",
      "\n",
      "Epoch 00007: loss improved from 7.64372 to 7.22284, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-07_loss-7.2228_val_loss-7.4088.h5\n",
      "Epoch 8/120\n",
      "\n",
      "Epoch 00008: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 321s 647ms/step - loss: 7.0471 - val_loss: 9.3487\n",
      "\n",
      "Epoch 00008: loss improved from 7.22284 to 7.04281, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-08_loss-7.0428_val_loss-9.3487.h5\n",
      "Epoch 9/120\n",
      "\n",
      "Epoch 00009: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 318s 641ms/step - loss: 6.7736 - val_loss: 8.5639\n",
      "\n",
      "Epoch 00009: loss improved from 7.04281 to 6.76816, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-09_loss-6.7682_val_loss-8.5639.h5\n",
      "Epoch 10/120\n",
      "\n",
      "Epoch 00010: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 325s 655ms/step - loss: 6.6471 - val_loss: 8.2836\n",
      "\n",
      "Epoch 00010: loss improved from 6.76816 to 6.64620, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-10_loss-6.6462_val_loss-8.2836.h5\n",
      "Epoch 11/120\n",
      "\n",
      "Epoch 00011: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 318s 642ms/step - loss: 6.3902 - val_loss: 7.8737\n",
      "\n",
      "Epoch 00011: loss improved from 6.64620 to 6.38508, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-11_loss-6.3851_val_loss-7.8737.h5\n",
      "Epoch 12/120\n",
      "\n",
      "Epoch 00012: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 321s 648ms/step - loss: 6.3885 - val_loss: 10.0802\n",
      "\n",
      "Epoch 00012: loss did not improve from 6.38508\n",
      "Epoch 13/120\n",
      "\n",
      "Epoch 00013: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 318s 641ms/step - loss: 6.1524 - val_loss: 7.8067\n",
      "\n",
      "Epoch 00013: loss improved from 6.38508 to 6.15270, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-13_loss-6.1527_val_loss-7.8067.h5\n",
      "Epoch 14/120\n",
      "\n",
      "Epoch 00014: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 319s 642ms/step - loss: 6.0650 - val_loss: 8.8796\n",
      "\n",
      "Epoch 00014: loss improved from 6.15270 to 6.05978, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-14_loss-6.0598_val_loss-8.8796.h5\n",
      "Epoch 15/120\n",
      "\n",
      "Epoch 00015: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 320s 645ms/step - loss: 5.9151 - val_loss: 8.3006\n",
      "\n",
      "Epoch 00015: loss improved from 6.05978 to 5.91244, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-15_loss-5.9124_val_loss-8.3006.h5\n",
      "Epoch 16/120\n",
      "\n",
      "Epoch 00016: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 318s 641ms/step - loss: 5.8776 - val_loss: 8.4184\n",
      "\n",
      "Epoch 00016: loss improved from 5.91244 to 5.87763, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-16_loss-5.8776_val_loss-8.4184.h5\n",
      "Epoch 17/120\n",
      "\n",
      "Epoch 00017: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 321s 646ms/step - loss: 5.7128 - val_loss: 8.9793\n",
      "\n",
      "Epoch 00017: loss improved from 5.87763 to 5.71230, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-17_loss-5.7123_val_loss-8.9793.h5\n",
      "Epoch 18/120\n",
      "\n",
      "Epoch 00018: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 319s 643ms/step - loss: 5.7898 - val_loss: 5.9446\n",
      "\n",
      "Epoch 00018: loss did not improve from 5.71230\n",
      "Epoch 19/120\n",
      "\n",
      "Epoch 00019: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 318s 641ms/step - loss: 5.5665 - val_loss: 6.7030\n",
      "\n",
      "Epoch 00019: loss improved from 5.71230 to 5.56619, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-19_loss-5.5662_val_loss-6.7030.h5\n",
      "Epoch 20/120\n",
      "\n",
      "Epoch 00020: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 319s 643ms/step - loss: 5.5186 - val_loss: 7.2419\n",
      "\n",
      "Epoch 00020: loss improved from 5.56619 to 5.51112, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-20_loss-5.5111_val_loss-7.2419.h5\n",
      "Epoch 21/120\n",
      "\n",
      "Epoch 00021: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 318s 641ms/step - loss: 5.3859 - val_loss: 5.6793\n",
      "\n",
      "Epoch 00021: loss improved from 5.51112 to 5.38881, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-21_loss-5.3888_val_loss-5.6793.h5\n",
      "Epoch 22/120\n",
      "\n",
      "Epoch 00022: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 320s 645ms/step - loss: 5.3045 - val_loss: 7.8791\n",
      "\n",
      "Epoch 00022: loss improved from 5.38881 to 5.29761, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-22_loss-5.2976_val_loss-7.8791.h5\n",
      "Epoch 23/120\n",
      "\n",
      "Epoch 00023: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 320s 645ms/step - loss: 5.1824 - val_loss: 6.0251\n",
      "\n",
      "Epoch 00023: loss improved from 5.29761 to 5.18402, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-23_loss-5.1840_val_loss-6.0251.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/120\n",
      "\n",
      "Epoch 00024: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 320s 646ms/step - loss: 5.1492 - val_loss: 7.1332\n",
      "\n",
      "Epoch 00024: loss improved from 5.18402 to 5.14118, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-24_loss-5.1412_val_loss-7.1332.h5\n",
      "Epoch 25/120\n",
      "\n",
      "Epoch 00025: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 316s 636ms/step - loss: 5.1065 - val_loss: 6.1756\n",
      "\n",
      "Epoch 00025: loss improved from 5.14118 to 5.09894, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-25_loss-5.0989_val_loss-6.1756.h5\n",
      "Epoch 26/120\n",
      "\n",
      "Epoch 00026: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 316s 637ms/step - loss: 4.9559 - val_loss: 7.1549\n",
      "\n",
      "Epoch 00026: loss improved from 5.09894 to 4.94725, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-26_loss-4.9473_val_loss-7.1549.h5\n",
      "Epoch 27/120\n",
      "\n",
      "Epoch 00027: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 320s 646ms/step - loss: 4.9949 - val_loss: 6.7566\n",
      "\n",
      "Epoch 00027: loss did not improve from 4.94725\n",
      "Epoch 28/120\n",
      "\n",
      "Epoch 00028: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 320s 646ms/step - loss: 4.8670 - val_loss: 5.3356\n",
      "\n",
      "Epoch 00028: loss improved from 4.94725 to 4.86601, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-28_loss-4.8660_val_loss-5.3356.h5\n",
      "Epoch 29/120\n",
      "\n",
      "Epoch 00029: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 320s 646ms/step - loss: 4.8748 - val_loss: 6.3304\n",
      "\n",
      "Epoch 00029: loss did not improve from 4.86601\n",
      "Epoch 30/120\n",
      "\n",
      "Epoch 00030: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 321s 646ms/step - loss: 4.6806 - val_loss: 5.0163\n",
      "\n",
      "Epoch 00030: loss improved from 4.86601 to 4.67680, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-30_loss-4.6768_val_loss-5.0163.h5\n",
      "Epoch 31/120\n",
      "\n",
      "Epoch 00031: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 317s 639ms/step - loss: 4.7208 - val_loss: 6.3768\n",
      "\n",
      "Epoch 00031: loss did not improve from 4.67680\n",
      "Epoch 32/120\n",
      "\n",
      "Epoch 00032: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 321s 646ms/step - loss: 4.6516 - val_loss: 4.9711\n",
      "\n",
      "Epoch 00032: loss improved from 4.67680 to 4.64816, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-32_loss-4.6482_val_loss-4.9711.h5\n",
      "Epoch 33/120\n",
      "\n",
      "Epoch 00033: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 319s 643ms/step - loss: 4.6314 - val_loss: 8.0028\n",
      "\n",
      "Epoch 00033: loss improved from 4.64816 to 4.63186, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-33_loss-4.6319_val_loss-8.0028.h5\n",
      "Epoch 34/120\n",
      "\n",
      "Epoch 00034: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 318s 641ms/step - loss: 4.5137 - val_loss: 4.9660\n",
      "\n",
      "Epoch 00034: loss improved from 4.63186 to 4.51303, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-34_loss-4.5130_val_loss-4.9660.h5\n",
      "Epoch 35/120\n",
      "\n",
      "Epoch 00035: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 318s 641ms/step - loss: 4.4303 - val_loss: 4.8581\n",
      "\n",
      "Epoch 00035: loss improved from 4.51303 to 4.43156, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-35_loss-4.4316_val_loss-4.8581.h5\n",
      "Epoch 36/120\n",
      "\n",
      "Epoch 00036: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 317s 639ms/step - loss: 4.3954 - val_loss: 4.6031\n",
      "\n",
      "Epoch 00036: loss improved from 4.43156 to 4.39548, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-36_loss-4.3955_val_loss-4.6031.h5\n",
      "Epoch 37/120\n",
      "\n",
      "Epoch 00037: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 322s 650ms/step - loss: 4.3771 - val_loss: 4.7984\n",
      "\n",
      "Epoch 00037: loss improved from 4.39548 to 4.37503, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-37_loss-4.3750_val_loss-4.7984.h5\n",
      "Epoch 38/120\n",
      "\n",
      "Epoch 00038: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 319s 642ms/step - loss: 4.3081 - val_loss: 4.3229\n",
      "\n",
      "Epoch 00038: loss improved from 4.37503 to 4.30710, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-38_loss-4.3071_val_loss-4.3229.h5\n",
      "Epoch 39/120\n",
      "\n",
      "Epoch 00039: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 318s 641ms/step - loss: 4.2666 - val_loss: 4.8604\n",
      "\n",
      "Epoch 00039: loss improved from 4.30710 to 4.26428, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-39_loss-4.2643_val_loss-4.8604.h5\n",
      "Epoch 40/120\n",
      "\n",
      "Epoch 00040: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 318s 641ms/step - loss: 4.3233 - val_loss: 4.4522\n",
      "\n",
      "Epoch 00040: loss did not improve from 4.26428\n",
      "Epoch 41/120\n",
      "\n",
      "Epoch 00041: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 318s 642ms/step - loss: 4.2653 - val_loss: 4.7720\n",
      "\n",
      "Epoch 00041: loss improved from 4.26428 to 4.25786, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-41_loss-4.2579_val_loss-4.7720.h5\n",
      "Epoch 42/120\n",
      "\n",
      "Epoch 00042: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 321s 647ms/step - loss: 4.2456 - val_loss: 4.7483\n",
      "\n",
      "Epoch 00042: loss improved from 4.25786 to 4.24290, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-42_loss-4.2429_val_loss-4.7483.h5\n",
      "Epoch 43/120\n",
      "\n",
      "Epoch 00043: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 317s 639ms/step - loss: 4.2167 - val_loss: 4.4985\n",
      "\n",
      "Epoch 00043: loss improved from 4.24290 to 4.21312, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-43_loss-4.2131_val_loss-4.4985.h5\n",
      "Epoch 44/120\n",
      "\n",
      "Epoch 00044: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 320s 645ms/step - loss: 4.2221 - val_loss: 3.9537\n",
      "\n",
      "Epoch 00044: loss did not improve from 4.21312\n",
      "Epoch 45/120\n",
      "\n",
      "Epoch 00045: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 319s 643ms/step - loss: 4.1094 - val_loss: 4.3601\n",
      "\n",
      "Epoch 00045: loss improved from 4.21312 to 4.10882, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-45_loss-4.1088_val_loss-4.3601.h5\n",
      "Epoch 46/120\n",
      "\n",
      "Epoch 00046: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 320s 646ms/step - loss: 4.0457 - val_loss: 5.5886\n",
      "\n",
      "Epoch 00046: loss improved from 4.10882 to 4.04010, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-46_loss-4.0401_val_loss-5.5886.h5\n",
      "Epoch 47/120\n",
      "\n",
      "Epoch 00047: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 323s 651ms/step - loss: 4.0709 - val_loss: 4.6400\n",
      "\n",
      "Epoch 00047: loss did not improve from 4.04010\n",
      "Epoch 48/120\n",
      "\n",
      "Epoch 00048: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 320s 645ms/step - loss: 4.0200 - val_loss: 5.5205\n",
      "\n",
      "Epoch 00048: loss improved from 4.04010 to 4.01835, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-48_loss-4.0184_val_loss-5.5205.h5\n",
      "Epoch 49/120\n",
      "\n",
      "Epoch 00049: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 320s 644ms/step - loss: 4.0336 - val_loss: 4.2136\n",
      "\n",
      "Epoch 00049: loss did not improve from 4.01835\n",
      "Epoch 50/120\n",
      "\n",
      "Epoch 00050: LearningRateScheduler setting learning rate to 0.001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "496/496 [==============================] - 318s 641ms/step - loss: 3.9942 - val_loss: 3.7511\n",
      "\n",
      "Epoch 00050: loss improved from 4.01835 to 3.99753, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-50_loss-3.9975_val_loss-3.7511.h5\n",
      "Epoch 51/120\n",
      "\n",
      "Epoch 00051: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 322s 649ms/step - loss: 4.0108 - val_loss: 4.1237\n",
      "\n",
      "Epoch 00051: loss did not improve from 3.99753\n",
      "Epoch 52/120\n",
      "\n",
      "Epoch 00052: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 318s 641ms/step - loss: 3.9727 - val_loss: 4.6090\n",
      "\n",
      "Epoch 00052: loss improved from 3.99753 to 3.97266, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-52_loss-3.9727_val_loss-4.6090.h5\n",
      "Epoch 53/120\n",
      "\n",
      "Epoch 00053: LearningRateScheduler setting learning rate to 0.001.\n",
      "  1/496 [..............................] - ETA: 2:51 - loss: 4.0360"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiaoxiao/anaconda3/envs/tf13/lib/python3.6/site-packages/keras/callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.326636). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "496/496 [==============================] - 319s 643ms/step - loss: 3.9322 - val_loss: 3.7611\n",
      "\n",
      "Epoch 00053: loss improved from 3.97266 to 3.92904, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-53_loss-3.9290_val_loss-3.7611.h5\n",
      "Epoch 54/120\n",
      "\n",
      "Epoch 00054: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 317s 639ms/step - loss: 3.9580 - val_loss: 4.4370\n",
      "\n",
      "Epoch 00054: loss did not improve from 3.92904\n",
      "Epoch 55/120\n",
      "\n",
      "Epoch 00055: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 321s 646ms/step - loss: 3.8751 - val_loss: 4.0441\n",
      "\n",
      "Epoch 00055: loss improved from 3.92904 to 3.86761, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-55_loss-3.8676_val_loss-4.0441.h5\n",
      "Epoch 56/120\n",
      "\n",
      "Epoch 00056: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 320s 645ms/step - loss: 3.8942 - val_loss: 3.5039\n",
      "\n",
      "Epoch 00056: loss did not improve from 3.86761\n",
      "Epoch 57/120\n",
      "\n",
      "Epoch 00057: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 317s 640ms/step - loss: 3.8355 - val_loss: 3.5629\n",
      "\n",
      "Epoch 00057: loss improved from 3.86761 to 3.84239, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-57_loss-3.8424_val_loss-3.5629.h5\n",
      "Epoch 58/120\n",
      "\n",
      "Epoch 00058: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 320s 646ms/step - loss: 3.8387 - val_loss: 4.0875\n",
      "\n",
      "Epoch 00058: loss improved from 3.84239 to 3.83481, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-58_loss-3.8348_val_loss-4.0875.h5\n",
      "Epoch 59/120\n",
      "\n",
      "Epoch 00059: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 319s 642ms/step - loss: 3.7848 - val_loss: 3.7346\n",
      "\n",
      "Epoch 00059: loss improved from 3.83481 to 3.78665, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-59_loss-3.7867_val_loss-3.7346.h5\n",
      "Epoch 60/120\n",
      "\n",
      "Epoch 00060: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 320s 645ms/step - loss: 3.8164 - val_loss: 3.5620\n",
      "\n",
      "Epoch 00060: loss did not improve from 3.78665\n",
      "Epoch 61/120\n",
      "\n",
      "Epoch 00061: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 320s 645ms/step - loss: 3.7730 - val_loss: 4.0519\n",
      "\n",
      "Epoch 00061: loss improved from 3.78665 to 3.77278, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-61_loss-3.7728_val_loss-4.0519.h5\n",
      "Epoch 62/120\n",
      "\n",
      "Epoch 00062: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 316s 637ms/step - loss: 3.7936 - val_loss: 4.0618\n",
      "\n",
      "Epoch 00062: loss did not improve from 3.77278\n",
      "Epoch 63/120\n",
      "\n",
      "Epoch 00063: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 319s 644ms/step - loss: 3.7321 - val_loss: 4.9338\n",
      "\n",
      "Epoch 00063: loss improved from 3.77278 to 3.73134, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-63_loss-3.7313_val_loss-4.9338.h5\n",
      "Epoch 64/120\n",
      "\n",
      "Epoch 00064: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 315s 635ms/step - loss: 3.7554 - val_loss: 3.6939\n",
      "\n",
      "Epoch 00064: loss did not improve from 3.73134\n",
      "Epoch 65/120\n",
      "\n",
      "Epoch 00065: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 313s 631ms/step - loss: 3.7485 - val_loss: 3.7090\n",
      "\n",
      "Epoch 00065: loss did not improve from 3.73134\n",
      "Epoch 66/120\n",
      "\n",
      "Epoch 00066: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 319s 642ms/step - loss: 3.7290 - val_loss: 4.1650\n",
      "\n",
      "Epoch 00066: loss improved from 3.73134 to 3.72426, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-66_loss-3.7243_val_loss-4.1650.h5\n",
      "Epoch 67/120\n",
      "\n",
      "Epoch 00067: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 313s 631ms/step - loss: 3.6636 - val_loss: 3.5731\n",
      "\n",
      "Epoch 00067: loss improved from 3.72426 to 3.66023, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-67_loss-3.6602_val_loss-3.5731.h5\n",
      "Epoch 68/120\n",
      "\n",
      "Epoch 00068: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 313s 631ms/step - loss: 3.6462 - val_loss: 3.5596\n",
      "\n",
      "Epoch 00068: loss improved from 3.66023 to 3.64607, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-68_loss-3.6461_val_loss-3.5596.h5\n",
      "Epoch 69/120\n",
      "\n",
      "Epoch 00069: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 311s 627ms/step - loss: 3.6442 - val_loss: 3.7504\n",
      "\n",
      "Epoch 00069: loss improved from 3.64607 to 3.64203, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-69_loss-3.6420_val_loss-3.7504.h5\n",
      "Epoch 70/120\n",
      "\n",
      "Epoch 00070: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 313s 631ms/step - loss: 3.6881 - val_loss: 4.1199\n",
      "\n",
      "Epoch 00070: loss did not improve from 3.64203\n",
      "Epoch 71/120\n",
      "\n",
      "Epoch 00071: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 313s 630ms/step - loss: 3.6103 - val_loss: 3.8172\n",
      "\n",
      "Epoch 00071: loss improved from 3.64203 to 3.60996, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-71_loss-3.6100_val_loss-3.8172.h5\n",
      "Epoch 72/120\n",
      "\n",
      "Epoch 00072: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 315s 635ms/step - loss: 3.6069 - val_loss: 3.3098\n",
      "\n",
      "Epoch 00072: loss improved from 3.60996 to 3.60277, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-72_loss-3.6028_val_loss-3.3098.h5\n",
      "Epoch 73/120\n",
      "\n",
      "Epoch 00073: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 318s 642ms/step - loss: 3.5999 - val_loss: 3.5541\n",
      "\n",
      "Epoch 00073: loss improved from 3.60277 to 3.59326, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-73_loss-3.5933_val_loss-3.5541.h5\n",
      "Epoch 74/120\n",
      "\n",
      "Epoch 00074: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 317s 638ms/step - loss: 3.5799 - val_loss: 3.8419\n",
      "\n",
      "Epoch 00074: loss improved from 3.59326 to 3.57527, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-74_loss-3.5753_val_loss-3.8419.h5\n",
      "Epoch 75/120\n",
      "\n",
      "Epoch 00075: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 313s 631ms/step - loss: 3.5454 - val_loss: 3.2253\n",
      "\n",
      "Epoch 00075: loss improved from 3.57527 to 3.54129, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-75_loss-3.5413_val_loss-3.2253.h5\n",
      "Epoch 76/120\n",
      "\n",
      "Epoch 00076: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 313s 632ms/step - loss: 3.5517 - val_loss: 3.9608\n",
      "\n",
      "Epoch 00076: loss did not improve from 3.54129\n",
      "Epoch 77/120\n",
      "\n",
      "Epoch 00077: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 315s 634ms/step - loss: 3.5463 - val_loss: 4.4167\n",
      "\n",
      "Epoch 00077: loss did not improve from 3.54129\n",
      "Epoch 78/120\n",
      "\n",
      "Epoch 00078: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 312s 629ms/step - loss: 3.5232 - val_loss: 3.3062\n",
      "\n",
      "Epoch 00078: loss improved from 3.54129 to 3.51724, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-78_loss-3.5172_val_loss-3.3062.h5\n",
      "Epoch 79/120\n",
      "\n",
      "Epoch 00079: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 316s 637ms/step - loss: 3.5232 - val_loss: 3.7295\n",
      "\n",
      "Epoch 00079: loss did not improve from 3.51724\n",
      "Epoch 80/120\n",
      "\n",
      "Epoch 00080: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 313s 632ms/step - loss: 3.5507 - val_loss: 4.2861\n",
      "\n",
      "Epoch 00080: loss did not improve from 3.51724\n",
      "Epoch 81/120\n",
      "\n",
      "Epoch 00081: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 311s 628ms/step - loss: 3.4257 - val_loss: 3.2107\n",
      "\n",
      "Epoch 00081: loss improved from 3.51724 to 3.42311, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-81_loss-3.4231_val_loss-3.2107.h5\n",
      "Epoch 82/120\n",
      "\n",
      "Epoch 00082: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 314s 633ms/step - loss: 3.5009 - val_loss: 3.6382\n",
      "\n",
      "Epoch 00082: loss did not improve from 3.42311\n",
      "Epoch 83/120\n",
      "\n",
      "Epoch 00083: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 313s 630ms/step - loss: 3.4315 - val_loss: 3.6220\n",
      "\n",
      "Epoch 00083: loss did not improve from 3.42311\n",
      "Epoch 84/120\n",
      "\n",
      "Epoch 00084: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 318s 641ms/step - loss: 3.4555 - val_loss: 3.6222\n",
      "\n",
      "Epoch 00084: loss did not improve from 3.42311\n",
      "Epoch 85/120\n",
      "\n",
      "Epoch 00085: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 313s 631ms/step - loss: 3.5110 - val_loss: 3.2508\n",
      "\n",
      "Epoch 00085: loss did not improve from 3.42311\n",
      "Epoch 86/120\n",
      "\n",
      "Epoch 00086: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 320s 646ms/step - loss: 3.3676 - val_loss: 3.4481\n",
      "\n",
      "Epoch 00086: loss improved from 3.42311 to 3.36917, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-86_loss-3.3692_val_loss-3.4481.h5\n",
      "Epoch 87/120\n",
      "\n",
      "Epoch 00087: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 313s 632ms/step - loss: 3.3828 - val_loss: 2.9944\n",
      "\n",
      "Epoch 00087: loss did not improve from 3.36917\n",
      "Epoch 88/120\n",
      "\n",
      "Epoch 00088: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 313s 632ms/step - loss: 3.4317 - val_loss: 3.0288\n",
      "\n",
      "Epoch 00088: loss did not improve from 3.36917\n",
      "Epoch 89/120\n",
      "\n",
      "Epoch 00089: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 314s 634ms/step - loss: 3.4451 - val_loss: 3.4856\n",
      "\n",
      "Epoch 00089: loss did not improve from 3.36917\n",
      "Epoch 90/120\n",
      "\n",
      "Epoch 00090: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 321s 648ms/step - loss: 3.3511 - val_loss: 6.6151\n",
      "\n",
      "Epoch 00090: loss improved from 3.36917 to 3.34752, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-90_loss-3.3475_val_loss-6.6151.h5\n",
      "Epoch 91/120\n",
      "\n",
      "Epoch 00091: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 312s 630ms/step - loss: 3.4118 - val_loss: 4.0058\n",
      "\n",
      "Epoch 00091: loss did not improve from 3.34752\n",
      "Epoch 92/120\n",
      "\n",
      "Epoch 00092: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 316s 638ms/step - loss: 3.3867 - val_loss: 4.1021\n",
      "\n",
      "Epoch 00092: loss did not improve from 3.34752\n",
      "Epoch 93/120\n",
      "\n",
      "Epoch 00093: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 312s 628ms/step - loss: 3.3165 - val_loss: 3.5459\n",
      "\n",
      "Epoch 00093: loss improved from 3.34752 to 3.31043, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-93_loss-3.3104_val_loss-3.5459.h5\n",
      "Epoch 94/120\n",
      "\n",
      "Epoch 00094: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 318s 640ms/step - loss: 3.4156 - val_loss: 2.9302\n",
      "\n",
      "Epoch 00094: loss did not improve from 3.31043\n",
      "Epoch 95/120\n",
      "\n",
      "Epoch 00095: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 315s 634ms/step - loss: 3.2845 - val_loss: 3.1451\n",
      "\n",
      "Epoch 00095: loss improved from 3.31043 to 3.28720, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-95_loss-3.2872_val_loss-3.1451.h5\n",
      "Epoch 96/120\n",
      "\n",
      "Epoch 00096: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 311s 627ms/step - loss: 3.3924 - val_loss: 4.5052\n",
      "\n",
      "Epoch 00096: loss did not improve from 3.28720\n",
      "Epoch 97/120\n",
      "\n",
      "Epoch 00097: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 323s 651ms/step - loss: 3.3433 - val_loss: 3.0883\n",
      "\n",
      "Epoch 00097: loss did not improve from 3.28720\n",
      "Epoch 98/120\n",
      "\n",
      "Epoch 00098: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 314s 633ms/step - loss: 3.3300 - val_loss: 3.9388\n",
      "\n",
      "Epoch 00098: loss did not improve from 3.28720\n",
      "Epoch 99/120\n",
      "\n",
      "Epoch 00099: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 313s 632ms/step - loss: 3.2712 - val_loss: 4.0588\n",
      "\n",
      "Epoch 00099: loss improved from 3.28720 to 3.27280, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-99_loss-3.2728_val_loss-4.0588.h5\n",
      "Epoch 100/120\n",
      "\n",
      "Epoch 00100: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 310s 626ms/step - loss: 3.2672 - val_loss: 3.3878\n",
      "\n",
      "Epoch 00100: loss improved from 3.27280 to 3.26429, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-100_loss-3.2643_val_loss-3.3878.h5\n",
      "Epoch 101/120\n",
      "\n",
      "Epoch 00101: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 312s 630ms/step - loss: 3.2793 - val_loss: 2.9354\n",
      "\n",
      "Epoch 00101: loss did not improve from 3.26429\n",
      "Epoch 102/120\n",
      "\n",
      "Epoch 00102: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 310s 624ms/step - loss: 3.2407 - val_loss: 3.4561\n",
      "\n",
      "Epoch 00102: loss improved from 3.26429 to 3.24456, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-102_loss-3.2446_val_loss-3.4561.h5\n",
      "Epoch 103/120\n",
      "\n",
      "Epoch 00103: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 310s 626ms/step - loss: 3.2227 - val_loss: 3.7614\n",
      "\n",
      "Epoch 00103: loss improved from 3.24456 to 3.22090, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-103_loss-3.2209_val_loss-3.7614.h5\n",
      "Epoch 104/120\n",
      "\n",
      "Epoch 00104: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 314s 634ms/step - loss: 3.2283 - val_loss: 3.4486\n",
      "\n",
      "Epoch 00104: loss did not improve from 3.22090\n",
      "Epoch 105/120\n",
      "\n",
      "Epoch 00105: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 320s 645ms/step - loss: 3.3606 - val_loss: 3.1460\n",
      "\n",
      "Epoch 00105: loss did not improve from 3.22090\n",
      "Epoch 106/120\n",
      "\n",
      "Epoch 00106: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 320s 645ms/step - loss: 3.1849 - val_loss: 3.0246\n",
      "\n",
      "Epoch 00106: loss improved from 3.22090 to 3.17662, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-106_loss-3.1766_val_loss-3.0246.h5\n",
      "Epoch 107/120\n",
      "\n",
      "Epoch 00107: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 318s 640ms/step - loss: 3.1943 - val_loss: 3.5920\n",
      "\n",
      "Epoch 00107: loss did not improve from 3.17662\n",
      "Epoch 108/120\n",
      "\n",
      "Epoch 00108: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 318s 641ms/step - loss: 3.2420 - val_loss: 3.5957\n",
      "\n",
      "Epoch 00108: loss did not improve from 3.17662\n",
      "Epoch 109/120\n",
      "\n",
      "Epoch 00109: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 325s 654ms/step - loss: 3.1938 - val_loss: 3.3224\n",
      "\n",
      "Epoch 00109: loss did not improve from 3.17662\n",
      "Epoch 110/120\n",
      "\n",
      "Epoch 00110: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 313s 632ms/step - loss: 3.2365 - val_loss: 2.7967\n",
      "\n",
      "Epoch 00110: loss did not improve from 3.17662\n",
      "Epoch 111/120\n",
      "\n",
      "Epoch 00111: LearningRateScheduler setting learning rate to 0.001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "496/496 [==============================] - 314s 634ms/step - loss: 3.1595 - val_loss: 3.6014\n",
      "\n",
      "Epoch 00111: loss improved from 3.17662 to 3.15809, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-111_loss-3.1581_val_loss-3.6014.h5\n",
      "Epoch 112/120\n",
      "\n",
      "Epoch 00112: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 312s 630ms/step - loss: 3.2012 - val_loss: 3.3816\n",
      "\n",
      "Epoch 00112: loss did not improve from 3.15809\n",
      "Epoch 113/120\n",
      "\n",
      "Epoch 00113: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 313s 631ms/step - loss: 3.1957 - val_loss: 3.6792\n",
      "\n",
      "Epoch 00113: loss did not improve from 3.15809\n",
      "Epoch 114/120\n",
      "\n",
      "Epoch 00114: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 317s 639ms/step - loss: 3.1936 - val_loss: 3.2414\n",
      "\n",
      "Epoch 00114: loss did not improve from 3.15809\n",
      "Epoch 115/120\n",
      "\n",
      "Epoch 00115: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 315s 635ms/step - loss: 3.2017 - val_loss: 3.8123\n",
      "\n",
      "Epoch 00115: loss did not improve from 3.15809\n",
      "Epoch 116/120\n",
      "\n",
      "Epoch 00116: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 321s 647ms/step - loss: 3.1565 - val_loss: 2.9669\n",
      "\n",
      "Epoch 00116: loss improved from 3.15809 to 3.15575, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-116_loss-3.1557_val_loss-2.9669.h5\n",
      "Epoch 117/120\n",
      "\n",
      "Epoch 00117: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 318s 640ms/step - loss: 3.1371 - val_loss: 3.0029\n",
      "\n",
      "Epoch 00117: loss improved from 3.15575 to 3.13191, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-117_loss-3.1319_val_loss-3.0029.h5\n",
      "Epoch 118/120\n",
      "\n",
      "Epoch 00118: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 312s 630ms/step - loss: 3.1380 - val_loss: 3.8727\n",
      "\n",
      "Epoch 00118: loss did not improve from 3.13191\n",
      "Epoch 119/120\n",
      "\n",
      "Epoch 00119: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 315s 635ms/step - loss: 3.1439 - val_loss: 4.0668\n",
      "\n",
      "Epoch 00119: loss did not improve from 3.13191\n",
      "Epoch 120/120\n",
      "\n",
      "Epoch 00120: LearningRateScheduler setting learning rate to 0.001.\n",
      "496/496 [==============================] - 312s 629ms/step - loss: 3.0907 - val_loss: 2.6933\n",
      "\n",
      "Epoch 00120: loss improved from 3.13191 to 3.08861, saving model to ./concat_multi_s3s4_id18_log/MobileNetv2_ssdLite_multi_concat_s3s4_epoch-120_loss-3.0886_val_loss-2.6933.h5\n"
     ]
    }
   ],
   "source": [
    "initial_epoch   = 0\n",
    "final_epoch     = 120\n",
    "steps_per_epoch = 496#6000\n",
    "val_dataset_size = 839\n",
    "history = model.fit_generator(generator=train_generator,\n",
    "                              steps_per_epoch=steps_per_epoch,\n",
    "                              epochs=final_epoch,\n",
    "                              callbacks=callbacks,\n",
    "                              validation_data=val_generator,\n",
    "                              validation_steps=ceil(val_dataset_size/batch_size),\n",
    "                              initial_epoch=initial_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_generator = val_dataset.generate(batch_size=2,\n",
    "                                         shuffle=True,\n",
    "                                         transformations=[convert_to_3_channels,\n",
    "                                                          resize],\n",
    "                                         label_encoder=None,\n",
    "                                         returns={'processed_images',\n",
    "                                                  'filenames',\n",
    "                                                  'inverse_transform',\n",
    "                                                  'original_images',\n",
    "                                                  'original_labels'},\n",
    "                                         keep_images_without_gt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data_index/data/pic/192.168.43.116_1596989028111_sub.pgm\n",
      "[[[ 5  5  5]\n",
      "  [ 3  3  3]\n",
      "  [ 0  0  0]\n",
      "  ...\n",
      "  [ 7  7  7]\n",
      "  [15 15 15]\n",
      "  [23 23 23]]\n",
      "\n",
      " [[ 1  1  1]\n",
      "  [ 5  5  5]\n",
      "  [ 2  2  2]\n",
      "  ...\n",
      "  [ 0  0  0]\n",
      "  [ 1  1  1]\n",
      "  [20 20 20]]\n",
      "\n",
      " [[ 0  0  0]\n",
      "  [ 4  4  4]\n",
      "  [17 17 17]\n",
      "  ...\n",
      "  [20 20 20]\n",
      "  [21 21 21]\n",
      "  [25 25 25]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 4  4  4]\n",
      "  [ 4  4  4]\n",
      "  [ 3  3  3]\n",
      "  ...\n",
      "  [ 1  1  1]\n",
      "  [ 1  1  1]\n",
      "  [ 1  1  1]]\n",
      "\n",
      " [[ 6  6  6]\n",
      "  [ 6  6  6]\n",
      "  [ 3  3  3]\n",
      "  ...\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  0]]\n",
      "\n",
      " [[ 3  3  3]\n",
      "  [ 4  4  4]\n",
      "  [ 6  6  6]\n",
      "  ...\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  0]\n",
      "  [ 1  1  1]]]\n",
      "------------------\n",
      "[[[ 65.1       65.1       65.1     ]\n",
      "  [ 68.9       68.9       68.9     ]\n",
      "  [ 75.166664  75.166664  75.166664]\n",
      "  ...\n",
      "  [113.166664 113.166664 113.166664]\n",
      "  [115.7      115.7      115.7     ]\n",
      "  [118.9      118.9      118.9     ]]\n",
      "\n",
      " [[ 84.16334   84.16334   84.16334 ]\n",
      "  [ 89.83      89.83      89.83    ]\n",
      "  [ 72.25      72.25      72.25    ]\n",
      "  ...\n",
      "  [118.183334 118.183334 118.183334]\n",
      "  [118.85     118.85     118.85    ]\n",
      "  [117.64     117.64     117.64    ]]\n",
      "\n",
      " [[105.333336 105.333336 105.333336]\n",
      "  [123.700005 123.700005 123.700005]\n",
      "  [ 69.83333   69.83333   69.83333 ]\n",
      "  ...\n",
      "  [121.333336 121.333336 121.333336]\n",
      "  [115.8      115.8      115.8     ]\n",
      "  [112.1      112.1      112.1     ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 39.466667  39.466667  39.466667]\n",
      "  [ 38.300003  38.300003  38.300003]\n",
      "  [ 36.583336  36.583336  36.583336]\n",
      "  ...\n",
      "  [ 12.        12.        12.      ]\n",
      "  [ 12.9       12.9       12.9     ]\n",
      "  [ 13.966666  13.966666  13.966666]]\n",
      "\n",
      " [[ 40.043335  40.043335  40.043335]\n",
      "  [ 38.22      38.22      38.22    ]\n",
      "  [ 36.55      36.55      36.55    ]\n",
      "  ...\n",
      "  [ 10.7       10.7       10.7     ]\n",
      "  [ 11.599999  11.599999  11.599999]\n",
      "  [ 12.956666  12.956666  12.956666]]\n",
      "\n",
      " [[ 37.966667  37.966667  37.966667]\n",
      "  [ 37.1       37.1       37.1     ]\n",
      "  [ 37.833332  37.833332  37.833332]\n",
      "  ...\n",
      "  [ 10.        10.        10.      ]\n",
      "  [ 10.9       10.9       10.9     ]\n",
      "  [ 12.933333  12.933333  12.933333]]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 5, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-0ccdd37e6eab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 2: Generate samples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbatch_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_filenames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_inverse_transforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_original_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_original_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;31m# Which batch item to look at\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 5, got 2)"
     ]
    }
   ],
   "source": [
    "# 2: Generate samples.\n",
    "\n",
    "batch_images, batch_filenames, batch_inverse_transforms, batch_original_images, batch_original_labels = next(predict_generator)\n",
    "\n",
    "i = 0 # Which batch item to look at\n",
    "\n",
    "print(\"Image:\", batch_filenames[i])\n",
    "print()\n",
    "print(\"Ground truth boxes:\\n\")\n",
    "print(np.array(batch_original_labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(batch_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4: Decode the raw predictions in `y_pred`.\n",
    "\n",
    "y_pred_decoded = decode_detections(y_pred,\n",
    "                                   confidence_thresh=0.5,\n",
    "                                   iou_threshold=0.4,\n",
    "                                   top_k=200,\n",
    "                                   normalize_coords=normalize_coords,\n",
    "                                   img_height=img_height,\n",
    "                                   img_width=img_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5: Convert the predictions for the original image.\n",
    "\n",
    "y_pred_decoded_inv = apply_inverse_transforms(y_pred_decoded, batch_inverse_transforms)\n",
    "\n",
    "np.set_printoptions(precision=2, suppress=True, linewidth=90)\n",
    "print(\"Predicted boxes:\\n\")\n",
    "print('   class   conf xmin   ymin   xmax   ymax')\n",
    "print(y_pred_decoded_inv[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5: Draw the predicted boxes onto the image\n",
    "\n",
    "# Set the colors for the bounding boxes\n",
    "colors = plt.cm.hsv(np.linspace(0, 1, n_classes+1)).tolist()\n",
    "classes = ['background','human','bicycle','truck','car','bus']\n",
    "\n",
    "plt.figure(figsize=(20,12))\n",
    "plt.imshow(batch_original_images[i])\n",
    "\n",
    "current_axis = plt.gca()\n",
    "\n",
    "for box in batch_original_labels[i]:\n",
    "    xmin = box[1]\n",
    "    ymin = box[2]\n",
    "    xmax = box[3]\n",
    "    ymax = box[4]\n",
    "    label = '{}'.format(classes[int(box[0])])\n",
    "    current_axis.add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color='green', fill=False, linewidth=2))  \n",
    "    current_axis.text(xmin, ymin, label, size='x-large', color='white', bbox={'facecolor':'green', 'alpha':1.0})\n",
    "\n",
    "for box in y_pred_decoded_inv[i]:\n",
    "    xmin = box[2]\n",
    "    ymin = box[3]\n",
    "    xmax = box[4]\n",
    "    ymax = box[5]\n",
    "    color = colors[int(box[0])]\n",
    "    label = '{}: {:.2f}'.format(classes[int(box[0])], box[1])\n",
    "    current_axis.add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color=color, fill=False, linewidth=2))  \n",
    "    current_axis.text(xmin, ymin, label, size='x-large', color='white', bbox={'facecolor':color, 'alpha':1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "192.168.43.211_1596962113120\n",
    "192.168.43.211_1596961348165"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
